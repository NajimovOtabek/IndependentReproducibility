{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "import time\n",
    "import ray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    X_train_C: np.ndarray,\n",
    "    X_train_N: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test_C: np.ndarray,\n",
    "    X_test_N: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    encoder: Union[OneHotEncoder, OrdinalEncoder] = None,\n",
    "    normalize: bool = False,\n",
    "    select: bool = False,\n",
    "    select_threshold: float = 1e-5,\n",
    "    select_params: Dict[str, any] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "):\n",
    "    X_train_C, X_train_N = X_train_C.astype(object), X_train_N.astype(np.float32)\n",
    "    X_test_C, X_test_N = X_test_C.astype(object), X_test_N.astype(np.float32)\n",
    "\n",
    "    if select:\n",
    "        _t = time.time()\n",
    "        try:\n",
    "            select_params = select_params or dict()\n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            svc = LinearSVC(\n",
    "                random_state=random_state, **select_params\n",
    "            ).fit(X=np.nan_to_num(scaler.transform(X_train_N)), y=y_train)\n",
    "            M = np.asarray([abs(r) > select_threshold for r in np.ravel(svc.coef_)])\n",
    "            C_num = C_num[M]\n",
    "            X_train_N, X_test_N = X_train_N[:, M], X_test_N[:, M] \n",
    "            log(f'Complete feature selection ({time.time() - _t:.2f}s): # Cat. = {len(C_cat)}; # Num. = {len(C_num)}')\n",
    "        except:\n",
    "            log(f'Failure in feature selection. Keep training without it. Caused by: \\n{tb.format_exc()}')\n",
    "    \n",
    "    if normalize:\n",
    "        _t = time.time()\n",
    "\n",
    "        try:\n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = np.nan_to_num(scaler.transform(X_train_N)).astype(np.float32)\n",
    "            X_test_N = np.nan_to_num(scaler.transform(X_test_N)).astype(np.float32)\n",
    "            log(f'Complete to normalizing numeric features ({time.time() - _t:.2f}s)')\n",
    "        except:\n",
    "            log(f'Failure in normalizing numeric features. Keep training without it. Caused by: \\n{tb.format_exc()}')\n",
    "\n",
    "    if oversample:\n",
    "        _t = time.time()\n",
    "\n",
    "        try:\n",
    "            ord_enc = OrdinalEncoder(dtype=np.float32).fit(X_train_C)\n",
    "            X_train = np.concatenate((ord_enc.transform(X_train_C), X_train_N), axis=1)\n",
    "            I_cat = np.arange(X_train_C.shape[1])\n",
    "            I_num = np.setdiff1d(np.arange(X_train.shape[1]), I_cat)\n",
    "            if len(C_cat):\n",
    "                sampler = SMOTENC(categorical_features=I_cat, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "            \n",
    "            X_train_C, X_train_N = X_train[:, I_cat], X_train[:, I_num]\n",
    "            X_train_C = ord_enc.inverse_transform(X_train_C)\n",
    "            log(f'Complete to oversampling ({time.time() - _t:.2f}s)')\n",
    "        except:\n",
    "            log(f'Failure in oversampling. Keep training without it. Caused by:  \\n{tb.format_exc()}')\n",
    "    \n",
    "    X_train_C, X_test_C = encoder.transform(X_train_C).astype(np.int32), encoder.transform(X_test_C).astype(np.int32)\n",
    "\n",
    "    if isinstance(encoder, OneHotEncoder):\n",
    "        categories = C_cat = list(encoder.get_feature_names_out(C_cat))\n",
    "    else:\n",
    "        categories = {k: {c: i for i, c in enumerate(v)} for k, v in zip(C_cat, encoder.categories_)}\n",
    "        \n",
    "    X_train_C = pd.DataFrame(X_train_C, columns=C_cat, dtype=np.int32)\n",
    "    X_train_N = pd.DataFrame(X_train_N, columns=C_num, dtype=np.float32)\n",
    "    X_test_C = pd.DataFrame(X_test_C, columns=C_cat, dtype=np.int32)\n",
    "    X_test_N = pd.DataFrame(X_test_N, columns=C_num, dtype=np.float32)\n",
    "\n",
    "    X_train = pd.concat((X_train_C, X_train_N), axis=1)\n",
    "    X_test = pd.concat((X_test_C, X_test_N), axis=1)\n",
    "\n",
    "    try:\n",
    "        _t = time.time()\n",
    "\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        log(f'Complete to model fitting ({time.time() - _t:.2f}s)')\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    except:\n",
    "        log(f'Failure in model fitting. Caused by: \\n{tb.format_exc()}')\n",
    "\n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: bool = False,\n",
    "    select_threshold: float = 1e-5,\n",
    "    select_params: Dict[str, any] = None,\n",
    "    oversample: bool = False,\n",
    "    onehot: bool = False,\n",
    "    random_state: int = None\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    # Encoding categorical features\n",
    "    if onehot:\n",
    "        encoder = OneHotEncoder(dtype=np.int32, sparse_output=False, drop=None).fit(X[C_cat].values)\n",
    "    else:\n",
    "        encoder = OrdinalEncoder(dtype=np.int32).fit(X[C_cat].values)\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_train_C, X_train_N = X_train[C_cat], X_train[C_num]\n",
    "\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "        X_test_C, X_test_N = X_test[C_cat], X_test[C_num]\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train_C=X_train_C.values,\n",
    "            X_train_N=X_train_N.values,\n",
    "            y_train=y_train,\n",
    "            X_test_C=X_test_C.values,\n",
    "            X_test_N=X_test_N.values,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            estimator=clone(estimator),\n",
    "            encoder=encoder,\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            select_threshold=select_threshold,\n",
    "            select_params=select_params,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "            X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "CROSS_VAL_BASE = partial(\n",
    "    cross_val,\n",
    "    path=os.path.join(PATH_INTERMEDIATE, 'eval'),\n",
    "    select=True,\n",
    "    normalize=True,\n",
    "    select_threshold=1e-3,\n",
    "    select_params=dict(tol=1e-4, max_iter=5000, dual=False, penalty='l1', loss='squared_hinge', C=1e-2),\n",
    "    oversample=True,\n",
    "    split='logo',\n",
    "    random_state=RANDOM_STATE,\n",
    "    onehot=True\n",
    ")\n",
    "\n",
    "#CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "OVERSAMPLE = [True, False]\n",
    "ESTIMATORS = {'xgb': ESTIMATOR_XGB, 'dummy': ESTIMATOR_DUMMY, 'rf': ESTIMATOR_RF}\n",
    "\n",
    "\n",
    "with on_ray(num_cpus=6):\n",
    "    for l, o, e in product(\n",
    "        CLS, OVERSAMPLE, ESTIMATORS\n",
    "    ):\n",
    "        name = f'{l}#{\"os\" if o else \"ns\"}#{e}'\n",
    "        p = os.path.join(PATH_INTERMEDIATE, 'feat', f'{l}.pkl')\n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        cats = X.columns[X.dtypes == object]\n",
    "        CROSS_VAL_BASE(\n",
    "            X=X, y=y, groups=groups, name=name,\n",
    "            estimator=ESTIMATORS[e], \n",
    "            categories=cats,\n",
    "            oversample=o\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_prob,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            ) if not is_same_y else np.nan\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_prob[:, 1], average=None\n",
    "        ) if not is_same_y else np.nan\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_prob[:, i], pos_label=c, average=None\n",
    "            ) \n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_prob, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_prob=y_prob[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "\n",
    "for f in os.listdir(DIR_EVAL):\n",
    "    l, o, m, s = f[:f.index('.pkl')].split('#')\n",
    "    res = load(os.path.join(DIR_EVAL, f))\n",
    "    X, y = res.X_test, res.y_test\n",
    "    y_pred = res.estimator.predict(X)\n",
    "    y_prob = res.estimator.predict_proba(X)\n",
    "    ev_test = evaluate(\n",
    "        y_true=y,\n",
    "        y_pred=y_pred,\n",
    "        y_prob=y_prob,\n",
    "        classes=[0, 1]\n",
    "    )\n",
    "    \n",
    "    X, y = res.X_train, res.y_train\n",
    "    y_pred = res.estimator.predict(X)\n",
    "    y_prob = res.estimator.predict_proba(X)\n",
    "    ev_train = evaluate(\n",
    "        y_true=y,\n",
    "        y_pred=y_pred,\n",
    "        y_prob=y_prob,\n",
    "        classes=[0, 1]\n",
    "    )\n",
    "    \n",
    "    RESULTS_EVAL.append({\n",
    "        'label': l,\n",
    "        'oversampling': o,\n",
    "        'alg': m,\n",
    "        'split': s,\n",
    "        'n_feature': len(X.columns),\n",
    "        **{\n",
    "            f'test_{k}': v for k, v in ev_test.items()\n",
    "        },\n",
    "        **{\n",
    "            f'train_{k}': v for k, v in ev_train.items()\n",
    "        }\n",
    "    })\n",
    "    \n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "    ['label', 'oversampling', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                oversampling=row.oversampling,\n",
    "                alg=row.alg,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "        ['n_feature', 'train_class_ratio', 'train_inst_0', 'train_inst_1', 'test_inst_0', 'test_inst_1', 'test_acc', 'test_f1_0' ,'test_f1_1', 'test_f1_macro', 'train_f1_0' ,'train_f1_1', 'train_f1_macro',]\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep='')\n",
    ").pivot(\n",
    "    index=['label', 'alg', 'oversampling'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if hasattr(estimator, 'model'):\n",
    "        estimator = estimator.model\n",
    "    \n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "\n",
    "for f in os.listdir(DIR_EVAL):\n",
    "    res = load(os.path.join(DIR_EVAL, f))\n",
    "    \n",
    "    f_norm = f[:f.index('.pkl')]\n",
    "    cv = f_norm[:f.rindex('#')]\n",
    "    \n",
    "    feat_imp = feature_importance(res.estimator)\n",
    "    if not feat_imp:\n",
    "        continue\n",
    "        \n",
    "    names, importance = feat_imp\n",
    "    d = pd.DataFrame(\n",
    "        importance.reshape(1, -1),\n",
    "        columns=names\n",
    "    )\n",
    "    IMPORTANCE_EVAL[cv].append(d)\n",
    "\n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for k, v in IMPORTANCE_EVAL.items():\n",
    "    l, o, a = k.split('#')\n",
    "    \n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        oversampling=o,\n",
    "        alg=a\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0)\n",
    "IMPORTANCE_SUMMARY.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 22 -u cm\n",
    "\n",
    "plots <- list()\n",
    "\n",
    "#for (l in c('valence', 'arousal', 'stress', 'disturbance')) {\n",
    "for (l in c( 'stress')) {\n",
    "    data <- IMPORTANCE_SUMMARY %>% filter(\n",
    "        (label == l) & (oversampling == 'os')\n",
    "    )\n",
    "\n",
    "    p_label <- ggplot() + geom_text(\n",
    "        aes(x=.5, y=.5),\n",
    "        label=str_to_title(l), \n",
    "        family='ssp', \n",
    "        fontface='bold',\n",
    "        size=4\n",
    "    ) + theme_void()\n",
    "\n",
    "    data_rf <- data %>% filter(\n",
    "        alg == 'rf'\n",
    "    )\n",
    "\n",
    "    p_rf <- ggplot(\n",
    "        data %>% filter(alg == 'rf') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='Random Forest'\n",
    "    ) + ylim(\n",
    "        c(0, 0.08)\n",
    "    )\n",
    "    p_xgb <- ggplot(\n",
    "        data %>% filter(alg == 'xgb') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='XGBoost'\n",
    "    ) + ylim(\n",
    "        c(0, 0.08)\n",
    "    )\n",
    "    plots[[paste(l, 'label', sep='_')]] <- p_label\n",
    "    plots[[paste(l, 'rf', sep='_')]] <- p_rf\n",
    "    plots[[paste(l, 'xgb', sep='_')]] <- p_xgb\n",
    "}\n",
    "\n",
    "#p <- plots$arousal_label + plots$valence_label\n",
    "#p <- p / (plots$arousal_rf | plots$arousal_xgb | plots$valence_rf | plots$valence_xgb)\n",
    "#p <- p / (plots$stress_label + plots$disturbance_label)\n",
    "#p <- p / (plots$stress_rf | plots$stress_xgb | plots$disturbance_rf | plots$disturbance_xgb)\n",
    "p <- plots$stress_label \n",
    "p <- p / (plots$stress_rf | plots$stress_xgb)\n",
    "\n",
    "\n",
    "p <- p + plot_layout(\n",
    "    heights=c(1.1, 10, 1.1, 10)\n",
    ")\n",
    "\n",
    "ggsave(paste('./fig/imp.pdf'), plot=p, width=26, height=20, unit='cm', device=cairo_pdf)\n",
    "print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
