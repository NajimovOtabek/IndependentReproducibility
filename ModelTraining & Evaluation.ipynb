{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_low_variance(agg_feature, threshold=.0000001):\n",
    "    agg_feature_non_zero_var = agg_feature.loc[:,agg_feature.var()>threshold]\n",
    "    num_removed = agg_feature.shape[1]-agg_feature_non_zero_var.shape[1]\n",
    "    print(f'{num_removed}/{agg_feature.shape[1]} features with variance < {threshold} removed')\n",
    "    return agg_feature_non_zero_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def remove_pairwise_corr(agg_feature_percent_missing, PAIRWISE_CORR_THRESHOLD=0.8, outcome_variable=None):\n",
    "    if outcome_variable is not None:\n",
    "        outcome_variable = pd.Series(outcome_variable, index=agg_feature_percent_missing.index, name=\"outcome\")\n",
    "        corr_with_outcome = pd.merge(outcome_variable, agg_feature_percent_missing, left_index=True, right_index=True).corr()[outcome_variable.name].abs().sort_values(ascending=False)\n",
    "        importance_order = corr_with_outcome.index[1:].tolist()\n",
    "        agg_feature_percent_missing = agg_feature_percent_missing[importance_order]\n",
    "\n",
    "    Matrix = agg_feature_percent_missing.corr().abs()\n",
    "    \n",
    "    upper_triangle = Matrix.where(np.triu(np.ones(Matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    correlated_features = set()\n",
    "    for feature in upper_triangle.columns:\n",
    "        highly_correlated = upper_triangle[feature][upper_triangle[feature] > PAIRWISE_CORR_THRESHOLD].index\n",
    "        correlated_features.update(highly_correlated)\n",
    "\n",
    "    kept_features = list(set(agg_feature_percent_missing.columns) - correlated_features)\n",
    "    print(f\"Pairwise Corr: kept only {len(kept_features)}/{len(agg_feature_percent_missing.columns)} features\")\n",
    "    return agg_feature_percent_missing[kept_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "class CustomCV:\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y, groups):\n",
    "        logo = LeaveOneGroupOut()\n",
    "\n",
    "        for train_users, test_users in logo.split(X, y, groups):\n",
    "            X_train_users, X_test_user = X.loc[train_users], X.loc[test_users]\n",
    "            y_train_users, y_test_user = y[train_users], y[test_users]\n",
    "            group_train_users, group_test_user = groups[train_users], groups[test_users]\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=self.n_splits) \n",
    "\n",
    "            for train_index, test_index in tscv.split(X_test_user):\n",
    "                X_train, X_test = pd.concat([X_train_users, X_test_user.iloc[train_index]]), X_test_user.iloc[test_index]\n",
    "                y_train, y_test = np.concatenate([y_train_users, y_test_user[train_index]]), y_test_user[test_index]\n",
    "\n",
    "                yield (X_train.index, X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    elif alg == 'TimeSeriesSplit':\n",
    "        splitter = TimeSeriesSplit(n_splits=n_splits)\n",
    "    elif alg == 'custom_cv':\n",
    "        splitter = CustomCV(n_splits=n_splits)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", \"TimeSeriesSplit\", \"custom_cv\" or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "#         # Removing low variance features\n",
    "#         X_train = exclude_low_variance(X_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "#         # Removing highly correlated features\n",
    "#         X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'proc', 'LABELS_PROC.csv'), index_col=['pcode','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use user speicifc mean threshold\n",
    "\n",
    "# LABELS_PROC['stress_user_mean'] = np.nan\n",
    "\n",
    "# for user in LABELS_PROC.index.get_level_values('pcode').unique():\n",
    "#     user_df = LABELS_PROC.loc[user].copy()\n",
    "#     user_mean = user_df['stress'].mean()\n",
    "#     user_df['stress_user_mean'] = user_df['stress'].apply(lambda x: 1 if x > user_mean else 0)\n",
    "#     LABELS_PROC.loc[user, 'stress_user_mean'] = user_df['stress_user_mean'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 21:01:04,868\tINFO worker.py:1432 -- Connecting to existing Ray cluster at address: 192.168.1.28:6379...\n",
      "2023-06-09 21:01:04,908\tINFO worker.py:1616 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:06] In progress: [dummy#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:06] Success: [dummy#1] Normalizing numeric features (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:06] In progress: [dummy#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:07] Success: [dummy#1] Training (1.58s).\n",
      "\u001b[2m\u001b[36m(_train pid=624396)\u001b[0m [23-06-09 21:01:10] In progress: [rf_os#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:11] Success: [rf_os#5] 1-th Feature selection (0.42s).\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m - # Orig. Feat.: 2784 (# Cat. = 0; # Num. = 2784)\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m - # Sel. Feat.: 234 (# Cat. = 0; # Num. = 234)\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:11] In progress: [rf_os#5] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:10] In progress: [rf_os#4] Normalizing numeric features.\u001b[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:11] Success: [rf_os#5] Normalizing numeric features (0.12s).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:06] In progress: [dummy#5] Training.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624396)\u001b[0m [23-06-09 21:01:11] Success: [rf_os#2] Oversampling (0.08s).\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:13] Success: [rf_os#4] Training (1.73s).\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:14] In progress: [xgb_os#5] 1-th Feature selection.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:16] Success: [lr_os#1] 1-th Feature selection (0.36s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m - # Orig. Feat.: 2784 (# Cat. = 0; # Num. = 2784)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m - # Sel. Feat.: 206 (# Cat. = 0; # Num. = 206)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:16] In progress: [lr_os#1] Oversampling.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:16] In progress: [lr_os#5] Normalizing numeric features.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:16] Success: [lr_os#5] Normalizing numeric features (0.11s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:14] In progress: [xgb_os#5] Training.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:14] Success: [xgb_os#5] Oversampling (0.03s).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:17] Success: [lr_os#5] Training (0.73s).\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:20] In progress: [svm_os#4] 1-th Feature selection.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:20] Success: [svm_os#5] 1-th Feature selection (0.43s).\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m - # Orig. Feat.: 2784 (# Cat. = 0; # Num. = 2784)\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m - # Sel. Feat.: 234 (# Cat. = 0; # Num. = 234)\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:20] In progress: [svm_os#5] Oversampling.\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:19] In progress: [svm_os#4] Normalizing numeric features.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:20] Success: [svm_os#4] Normalizing numeric features (0.13s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:20] In progress: [svm_os#5] Training.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:20] Success: [svm_os#5] Oversampling (0.03s).\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:23] Success: [svm_os#4] Training (2.83s).\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:26] In progress: [mlp_os#2] 1-th Feature selection.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:26] Success: [mlp_os#5] 1-th Feature selection (0.43s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m - # Orig. Feat.: 2784 (# Cat. = 0; # Num. = 2784)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m - # Sel. Feat.: 234 (# Cat. = 0; # Num. = 234)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:26] In progress: [mlp_os#5] Oversampling.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:26] In progress: [mlp_os#5] Normalizing numeric features.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:26] Success: [mlp_os#2] Normalizing numeric features (0.10s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:26] In progress: [mlp_os#5] Training.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624376)\u001b[0m [23-06-09 21:01:26] Success: [mlp_os#5] Oversampling (0.10s).\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:31] Success: [mlp_os#4] Training (4.45s).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624383)\u001b[0m [23-06-09 21:01:32] In progress: [adab_os#4] 1-th Feature selection.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:33] Success: [adab_os#5] 1-th Feature selection (0.43s).\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m - # Orig. Feat.: 2784 (# Cat. = 0; # Num. = 2784)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m - # Sel. Feat.: 234 (# Cat. = 0; # Num. = 234)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:33] In progress: [adab_os#5] Oversampling.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:32] In progress: [adab_os#2] Normalizing numeric features.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624383)\u001b[0m [23-06-09 21:01:32] Success: [adab_os#4] Normalizing numeric features (0.11s).\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:33] In progress: [adab_os#5] Training.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=620796)\u001b[0m [23-06-09 21:01:33] Success: [adab_os#5] Oversampling (0.10s).\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=624409)\u001b[0m [23-06-09 21:01:34] Success: [adab_os#2] Training (1.10s).\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "ESTIMATOR_LR = LogisticRegression(random_state = RANDOM_STATE, max_iter=500 )\n",
    "ESTIMATOR_KNN = KNeighborsClassifier()\n",
    "ESTIMATOR_SVM = SVC(probability=True)\n",
    "ESTIMATOR_GP = GaussianProcessClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_DT = DecisionTreeClassifier(random_state = RANDOM_STATE)\n",
    "ESTIMATOR_MLP = MLPClassifier(random_state=RANDOM_STATE, max_iter=2000)\n",
    "ESTIMATOR_ADAB = AdaBoostClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_GNB = GaussianNB()\n",
    "ESTIMATOR_QDA = QuadraticDiscriminantAnalysis()\n",
    "  \n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \n",
    "#         estimator=LogisticRegression(\n",
    "#         penalty='l1' \n",
    "#         ,solver='liblinear'\n",
    "#         , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "#     ),\n",
    "    threshold=1e-5\n",
    "#     threshold = 0.005\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_DUMMY),\n",
    "        oversample=False,\n",
    "        select=None,\n",
    "        name='dummy'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "#         select=None,\n",
    "        name='rf_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "#         select=None,\n",
    "        name='xgb_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_LR),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "#         select=None,\n",
    "\n",
    "        name='lr_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_KNN),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='knn_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_SVM),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='svm_os'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_GP),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gp_os'\n",
    "#     ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_DT),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='dt_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_MLP),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='mlp_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_ADAB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='adab_os'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_GNB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gnb_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_QDA),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='qda_os'\n",
    "#     )\n",
    "\n",
    "]\n",
    "\n",
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress.pkl')\n",
    "par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', 'stress')\n",
    "\n",
    "if os.path.isdir(par_dir):\n",
    "    # Get a list of all the files in the folder\n",
    "    files = os.listdir(par_dir)\n",
    "\n",
    "    # Delete all the files in the folder\n",
    "    for file in files:\n",
    "        if file !='.ipynb_checkpoints':\n",
    "            os.remove(os.path.join(par_dir, file))\n",
    "os.makedirs(par_dir, exist_ok=True)\n",
    "\n",
    "#with on_ray(num_cpus=6):\n",
    "with on_ray():\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):       \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        ################################################\n",
    "        #Use user speicifc mean threshold\n",
    "#         y =LABELS_PROC['stress_user_mean'].to_numpy()\n",
    "        #Use fixed threshold\n",
    "#         y =LABELS_PROC['stress_fixed'].to_numpy()\n",
    "        #Use three categories (fixed threshold)\n",
    "#        y =LABELS_PROC['stress_fixed_tri'].to_numpy()\n",
    "\n",
    "        \n",
    "        #The following code is designed for reordering for the sake of time series split\n",
    "        #################################################\n",
    "        #Convert them into a pandas DataFrame\n",
    "        df = pd.DataFrame({'user_id': groups, 'datetime': pd.to_datetime(datetimes)})\n",
    "\n",
    "        # Normalize the datetime for each user\n",
    "        df['datetime'] = df.groupby('user_id')['datetime'].transform(lambda x: x - x.min().normalize())\n",
    "\n",
    "        # Now you can sort the DataFrame by user_id and datetime\n",
    "        df = df.sort_values(by=['datetime'])\n",
    "\n",
    "        # Now, groups and datetimes can be updated\n",
    "        groups = df['user_id'].to_numpy()\n",
    "        datetimes = df['datetime'].to_numpy()\n",
    "\n",
    "        # Get the indices that would sort the dataframe\n",
    "        sorted_indices = df.sort_values(by=['datetime']).index\n",
    "\n",
    "        # Use these indices to reorder X and y\n",
    "        X = X.loc[sorted_indices]\n",
    "        y = y[sorted_indices]\n",
    "        ###################################################\n",
    "        \n",
    "\n",
    "        #The following code is for only using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[~X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[~y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[~groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        #The following code is for excluding using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        \n",
    "        \n",
    "        ###########################################\n",
    "        #The following code is for similar-user model\n",
    "        ###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "        ###########################################\n",
    "        #Remove low frequency features\n",
    "#         mask = ['CAE#', 'MED#', 'ONF#', 'PWS#', 'RNG#','MSG#' ]\n",
    "#         X = X.loc[:, [all(m not in str(x) for m in mask) for x in X.columns]]\n",
    "\n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([ feat_pif, feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "        #The following code is for calculating aggregated features\n",
    "        ########################################################################\n",
    "        # Define a function to split the column name into sensor and attribute\n",
    "#         def split_column_name(col_name):\n",
    "#             parts = col_name.rsplit(\"#\", 1)  # Split on last occurrence of '#'\n",
    "#             return parts[0]  # This gives you 'Sensor#Attribute'\n",
    "\n",
    "#         # Get a list of unique sensor-attribute combinations\n",
    "#         df=feat_today_sensor\n",
    "#         sensor_attributes = df.columns.map(split_column_name).unique()\n",
    "\n",
    "#         # Create a list to hold the aggregated results\n",
    "#         agg_results = []\n",
    "\n",
    "#         # Loop over each sensor-attribute, select the appropriate columns, compute the mean and std\n",
    "#         for sensor_attribute in sensor_attributes:\n",
    "#             # Select columns for this sensor-attribute\n",
    "#             cols_to_aggregate = [col for col in df.columns if col.startswith(sensor_attribute)]\n",
    "#             # Compute the mean and std and store in the new DataFrame\n",
    "#             agg_results.append(df[cols_to_aggregate].mean(axis=1).rename(sensor_attribute + '|'+ 'MEAN'))\n",
    "#             agg_results.append(df[cols_to_aggregate].std(axis=1).rename(sensor_attribute + '|'+'STD'))\n",
    "\n",
    "#         # Concatenate all the results into a single DataFrame\n",
    "#         agg_feature = pd.concat(agg_results, axis=1)\n",
    "        #########################################################################\n",
    "        feat_final = pd.concat([  feat_today_sensor ],axis=1)\n",
    "        X = feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups,\n",
    "            path=par_dir,\n",
    "            normalize=True,\n",
    "            split='groupk',\n",
    "            categories=cats,\n",
    "            split_params={'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation coefficient\n",
    "# r = np.corrcoef(feat_current_ESM['ESM#LastLabel_3'], y)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_prob,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            ) if not is_same_y else np.nan\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_prob[:, 1], average=None\n",
    "        ) if not is_same_y else np.nan\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_prob[:, i], pos_label=c, average=None\n",
    "            ) \n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_prob, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_prob=y_prob[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>split</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_class_ratio</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>...</th>\n",
       "      <th>train_pre_1</th>\n",
       "      <th>train_rec_1</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_roauc</th>\n",
       "      <th>train_prauc_0</th>\n",
       "      <th>train_prauc_ref_0</th>\n",
       "      <th>train_prauc_1</th>\n",
       "      <th>train_prauc_ref_1</th>\n",
       "      <th>train_log_loss</th>\n",
       "      <th>train_brier_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>knn_os</td>\n",
       "      <td>5</td>\n",
       "      <td>234</td>\n",
       "      <td>508</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>97</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826296</td>\n",
       "      <td>0.782727</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.891651</td>\n",
       "      <td>0.863249</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.871849</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.390747</td>\n",
       "      <td>0.131364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "      <td>523</td>\n",
       "      <td>253</td>\n",
       "      <td>270</td>\n",
       "      <td>0.937037</td>\n",
       "      <td>152</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784069</td>\n",
       "      <td>0.752995</td>\n",
       "      <td>0.768218</td>\n",
       "      <td>0.857263</td>\n",
       "      <td>0.851929</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866718</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.680696</td>\n",
       "      <td>0.243817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>mlp_os</td>\n",
       "      <td>3</td>\n",
       "      <td>199</td>\n",
       "      <td>530</td>\n",
       "      <td>251</td>\n",
       "      <td>279</td>\n",
       "      <td>0.899642</td>\n",
       "      <td>135</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991636</td>\n",
       "      <td>0.995800</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.999808</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.020475</td>\n",
       "      <td>0.003911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>mlp_os</td>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "      <td>523</td>\n",
       "      <td>253</td>\n",
       "      <td>270</td>\n",
       "      <td>0.937037</td>\n",
       "      <td>151</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998157</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.012705</td>\n",
       "      <td>0.001873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>lr_os</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>515</td>\n",
       "      <td>244</td>\n",
       "      <td>271</td>\n",
       "      <td>0.900369</td>\n",
       "      <td>145</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787963</td>\n",
       "      <td>0.785055</td>\n",
       "      <td>0.786506</td>\n",
       "      <td>0.876160</td>\n",
       "      <td>0.875286</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.880272</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.438052</td>\n",
       "      <td>0.143481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label      alg split  n_feature  test_inst  test_inst_0  test_inst_1  \\\n",
       "0  stress   knn_os     5        234        508          253          255   \n",
       "1  stress  adab_os     1        206        523          253          270   \n",
       "2  stress   mlp_os     3        199        530          251          279   \n",
       "3  stress   mlp_os     1        206        523          253          270   \n",
       "4  stress    lr_os     2        200        515          244          271   \n",
       "\n",
       "   test_class_ratio  test_true_0_pred_0  test_true_0_pred_1  ...  train_pre_1  \\\n",
       "0          0.992157                  97                 156  ...     0.826296   \n",
       "1          0.937037                 152                 101  ...     0.784069   \n",
       "2          0.899642                 135                 116  ...     1.000000   \n",
       "3          0.937037                 151                 102  ...     1.000000   \n",
       "4          0.900369                 145                  99  ...     0.787963   \n",
       "\n",
       "   train_rec_1  train_f1_1  train_roauc  train_prauc_0  train_prauc_ref_0  \\\n",
       "0     0.782727    0.803922     0.891651       0.863249                0.5   \n",
       "1     0.752995    0.768218     0.857263       0.851929                0.5   \n",
       "2     0.991636    0.995800     0.999775       0.999753                0.5   \n",
       "3     0.998157    0.999077     0.999959       0.999958                0.5   \n",
       "4     0.785055    0.786506     0.876160       0.875286                0.5   \n",
       "\n",
       "   train_prauc_1  train_prauc_ref_1  train_log_loss  train_brier_loss  \n",
       "0       0.871849                0.5        0.390747          0.131364  \n",
       "1       0.866718                0.5        0.680696          0.243817  \n",
       "2       0.999808                0.5        0.020475          0.003911  \n",
       "3       0.999961                0.5        0.012705          0.001873  \n",
       "4       0.880272                0.5        0.438052          0.143481  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "#for l in ['valence', 'arousal', 'disturbance', 'stress']:\n",
    "for l in [ 'stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        if f == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        model, pid = f[:f.index('.pkl')].split('#')\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "        X, y = res.X_test, res.y_test\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_test = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        X, y = res.X_train, res.y_train\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_train = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        RESULTS_EVAL.append({\n",
    "            'label': l,\n",
    "            'alg': model,\n",
    "            'split': pid,\n",
    "            'n_feature': len(X.columns),\n",
    "            **{\n",
    "                f'test_{k}': v for k, v in ev_test.items()\n",
    "            },\n",
    "            **{\n",
    "                f'train_{k}': v for k, v in ev_train.items()\n",
    "            }\n",
    "        })\n",
    "    \n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_xgbos = RESULTS_EVAL[RESULTS_EVAL['alg']=='lr_os']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_bac</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_roauc</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>test_true_1_pred_0</th>\n",
       "      <th>test_true_1_pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>515</td>\n",
       "      <td>244</td>\n",
       "      <td>271</td>\n",
       "      <td>0.596117</td>\n",
       "      <td>0.596024</td>\n",
       "      <td>0.595676</td>\n",
       "      <td>0.622043</td>\n",
       "      <td>145</td>\n",
       "      <td>99</td>\n",
       "      <td>109</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>530</td>\n",
       "      <td>251</td>\n",
       "      <td>279</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.524583</td>\n",
       "      <td>0.523717</td>\n",
       "      <td>0.549901</td>\n",
       "      <td>114</td>\n",
       "      <td>137</td>\n",
       "      <td>113</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>523</td>\n",
       "      <td>253</td>\n",
       "      <td>270</td>\n",
       "      <td>0.521989</td>\n",
       "      <td>0.524843</td>\n",
       "      <td>0.519584</td>\n",
       "      <td>0.531460</td>\n",
       "      <td>155</td>\n",
       "      <td>98</td>\n",
       "      <td>152</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>508</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.589863</td>\n",
       "      <td>0.577283</td>\n",
       "      <td>0.644106</td>\n",
       "      <td>105</td>\n",
       "      <td>148</td>\n",
       "      <td>60</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>543</td>\n",
       "      <td>263</td>\n",
       "      <td>280</td>\n",
       "      <td>0.554328</td>\n",
       "      <td>0.554468</td>\n",
       "      <td>0.554254</td>\n",
       "      <td>0.570342</td>\n",
       "      <td>147</td>\n",
       "      <td>116</td>\n",
       "      <td>126</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  test_inst  test_inst_0  test_inst_1  test_acc  test_bac  \\\n",
       "4      2        515          244          271  0.596117  0.596024   \n",
       "6      3        530          251          279  0.528302  0.524583   \n",
       "28     1        523          253          270  0.521989  0.524843   \n",
       "38     5        508          253          255  0.590551  0.589863   \n",
       "43     4        543          263          280  0.554328  0.554468   \n",
       "\n",
       "    test_f1_macro  test_roauc  test_true_0_pred_0  test_true_0_pred_1  \\\n",
       "4        0.595676    0.622043                 145                  99   \n",
       "6        0.523717    0.549901                 114                 137   \n",
       "28       0.519584    0.531460                 155                  98   \n",
       "38       0.577283    0.644106                 105                 148   \n",
       "43       0.554254    0.570342                 147                 116   \n",
       "\n",
       "    test_true_1_pred_0  test_true_1_pred_1  \n",
       "4                  109                 162  \n",
       "6                  113                 166  \n",
       "28                 152                 118  \n",
       "38                  60                 195  \n",
       "43                 126                 154  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_xgbos[['split','test_inst','test_inst_0','test_inst_1','test_acc','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "       'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_performance = RESULTS_xgbos['test_f1_macro'] >= 0.761\n",
    "# RESULTS_xgbos_high_performance = RESULTS_xgbos[mask_performance]\n",
    "# RESULTS_xgbos_low_performance = RESULTS_xgbos[~mask_performance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS_xgbos_low_performance[['split','test_inst','test_inst_0','test_inst_1','test_acc','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "#        'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS_xgbos_high_performance[['split','test_inst','test_inst_0','test_inst_1','test_acc','test_bac','test_f1_macro','test_roauc','test_true_0_pred_0',\n",
    "#        'test_true_0_pred_1', 'test_true_1_pred_0', 'test_true_1_pred_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code is for cluster analysis\n",
    "# pcode_to_cluster = dict(zip(similar_user['pcode'], similar_user['cluster']))\n",
    "# RESULTS_EVAL['cluster'] = RESULTS_EVAL['split'].map(pcode_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>metric</th>\n",
       "      <th>n</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>value_count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>med</th>\n",
       "      <th>range</th>\n",
       "      <th>conf.</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>split</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1:1, 4:1, 5:1, 2:1, 3:1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>n_feature</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>206.8</td>\n",
       "      <td>15.706686</td>\n",
       "      <td>200.0</td>\n",
       "      <td>(195, 234)</td>\n",
       "      <td>(187.29757287386508, 226.30242712613494)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>test_inst</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>523.8</td>\n",
       "      <td>13.553597</td>\n",
       "      <td>523.0</td>\n",
       "      <td>(508, 543)</td>\n",
       "      <td>(506.97098617592883, 540.6290138240711)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>test_inst_0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>252.8</td>\n",
       "      <td>6.797058</td>\n",
       "      <td>253.0</td>\n",
       "      <td>(244, 263)</td>\n",
       "      <td>(244.3603375552743, 261.23966244472575)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>adab_os</td>\n",
       "      <td>test_inst_1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>10.024969</td>\n",
       "      <td>271.0</td>\n",
       "      <td>(255, 280)</td>\n",
       "      <td>(258.55235712330426, 283.44764287669574)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label      alg       metric  n  cardinality              value_count  \\\n",
       "0  stress  adab_os        split  5          5.0  1:1, 4:1, 5:1, 2:1, 3:1   \n",
       "1  stress  adab_os    n_feature  5          NaN                      NaN   \n",
       "2  stress  adab_os    test_inst  5          NaN                      NaN   \n",
       "3  stress  adab_os  test_inst_0  5          NaN                      NaN   \n",
       "4  stress  adab_os  test_inst_1  5          NaN                      NaN   \n",
       "\n",
       "      sum   mean         SD    med       range  \\\n",
       "0     NaN    NaN        NaN    NaN         NaN   \n",
       "1  1034.0  206.8  15.706686  200.0  (195, 234)   \n",
       "2  2619.0  523.8  13.553597  523.0  (508, 543)   \n",
       "3  1264.0  252.8   6.797058  253.0  (244, 263)   \n",
       "4  1355.0  271.0  10.024969  271.0  (255, 280)   \n",
       "\n",
       "                                      conf.  nan_count  \n",
       "0                                       NaN        NaN  \n",
       "1  (187.29757287386508, 226.30242712613494)        0.0  \n",
       "2   (506.97098617592883, 540.6290138240711)        0.0  \n",
       "3   (244.3603375552743, 261.23966244472575)        0.0  \n",
       "4  (258.55235712330426, 283.44764287669574)        0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "#    ['label', 'alg', 'cluster']\n",
    "     ['label', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                alg=row.alg,\n",
    "#                 cluster = row.cluster,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">mean_sd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_bac</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_roauc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">stress</th>\n",
       "      <th>dummy</th>\n",
       "      <td>2784.0 (0.0)</td>\n",
       "      <td>0.517 (0.01)</td>\n",
       "      <td>0.5 (0.0)</td>\n",
       "      <td>0.341 (0.004)</td>\n",
       "      <td>0.5 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adab_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.542 (0.072)</td>\n",
       "      <td>0.542 (0.072)</td>\n",
       "      <td>0.538 (0.069)</td>\n",
       "      <td>0.572 (0.084)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.543 (0.024)</td>\n",
       "      <td>0.542 (0.024)</td>\n",
       "      <td>0.541 (0.023)</td>\n",
       "      <td>0.542 (0.024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.544 (0.029)</td>\n",
       "      <td>0.544 (0.028)</td>\n",
       "      <td>0.541 (0.031)</td>\n",
       "      <td>0.555 (0.034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.558 (0.034)</td>\n",
       "      <td>0.558 (0.034)</td>\n",
       "      <td>0.554 (0.033)</td>\n",
       "      <td>0.584 (0.048)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.562 (0.044)</td>\n",
       "      <td>0.562 (0.043)</td>\n",
       "      <td>0.559 (0.041)</td>\n",
       "      <td>0.582 (0.057)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.548 (0.033)</td>\n",
       "      <td>0.549 (0.032)</td>\n",
       "      <td>0.544 (0.033)</td>\n",
       "      <td>0.574 (0.039)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.585 (0.03)</td>\n",
       "      <td>0.585 (0.028)</td>\n",
       "      <td>0.576 (0.035)</td>\n",
       "      <td>0.619 (0.027)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_os</th>\n",
       "      <td>206.8 (15.707)</td>\n",
       "      <td>0.535 (0.029)</td>\n",
       "      <td>0.536 (0.026)</td>\n",
       "      <td>0.531 (0.032)</td>\n",
       "      <td>0.559 (0.027)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean_sd                                               \\\n",
       "metric               n_feature       test_acc       test_bac  test_f1_macro   \n",
       "label  alg                                                                    \n",
       "stress dummy      2784.0 (0.0)   0.517 (0.01)      0.5 (0.0)  0.341 (0.004)   \n",
       "       adab_os  206.8 (15.707)  0.542 (0.072)  0.542 (0.072)  0.538 (0.069)   \n",
       "       dt_os    206.8 (15.707)  0.543 (0.024)  0.542 (0.024)  0.541 (0.023)   \n",
       "       knn_os   206.8 (15.707)  0.544 (0.029)  0.544 (0.028)  0.541 (0.031)   \n",
       "       lr_os    206.8 (15.707)  0.558 (0.034)  0.558 (0.034)  0.554 (0.033)   \n",
       "       mlp_os   206.8 (15.707)  0.562 (0.044)  0.562 (0.043)  0.559 (0.041)   \n",
       "       rf_os    206.8 (15.707)  0.548 (0.033)  0.549 (0.032)  0.544 (0.033)   \n",
       "       svm_os   206.8 (15.707)   0.585 (0.03)  0.585 (0.028)  0.576 (0.035)   \n",
       "       xgb_os   206.8 (15.707)  0.535 (0.029)  0.536 (0.026)  0.531 (0.032)   \n",
       "\n",
       "                               \n",
       "metric             test_roauc  \n",
       "label  alg                     \n",
       "stress dummy        0.5 (0.0)  \n",
       "       adab_os  0.572 (0.084)  \n",
       "       dt_os    0.542 (0.024)  \n",
       "       knn_os   0.555 (0.034)  \n",
       "       lr_os    0.584 (0.048)  \n",
       "       mlp_os   0.582 (0.057)  \n",
       "       rf_os    0.574 (0.039)  \n",
       "       svm_os   0.619 (0.027)  \n",
       "       xgb_os   0.559 (0.027)  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "        ['n_feature','test_acc' ,'test_bac',  'test_f1_macro','test_roauc']\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep=''),\n",
    ").pivot(\n",
    "    index=['label', 'alg'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "\n",
    "# separate rows where 'alg' is 'dummy' and 'alg' is not 'dummy'\n",
    "df_dummy = SUB_SUMMARY_EVAL[SUB_SUMMARY_EVAL.index.get_level_values('alg') == 'dummy']\n",
    "df_others = SUB_SUMMARY_EVAL[SUB_SUMMARY_EVAL.index.get_level_values('alg') != 'dummy']\n",
    "\n",
    "# concatenate them ensuring that 'dummy' rows are always at the top for each group\n",
    "SUB_SUMMARY_EVAL = pd.concat([df_dummy, df_others])\n",
    "\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "# for l in ['valence', 'arousal', 'disturbance', 'stress']:\n",
    "for l in ['stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        if f!='.ipynb_checkpoints':\n",
    "            res = load(os.path.join(dir_l, f))\n",
    "\n",
    "            f_norm = f[:f.index('.pkl')]\n",
    "            alg = f_norm[:f.rindex('#')]\n",
    "\n",
    "            feat_imp = feature_importance(res.estimator)\n",
    "            if not feat_imp:\n",
    "                continue\n",
    "\n",
    "            names, importance = feat_imp\n",
    "            new_names = []\n",
    "            for n in names:\n",
    "                for c in res.categories:\n",
    "                    n = n.replace(f'{c}_', f'{c}=')\n",
    "                new_names.append(n)\n",
    "\n",
    "            d = pd.DataFrame(\n",
    "                importance.reshape(1, -1),\n",
    "                columns=new_names\n",
    "            )\n",
    "            IMPORTANCE_EVAL[(l, alg)].append(d)\n",
    "        \n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for (l, alg), v in IMPORTANCE_EVAL.items():\n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        alg=alg\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n",
      "✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n",
      "✔ tibble  3.1.8      ✔ dplyr   1.0.10\n",
      "✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n",
      "✔ readr   2.1.3      ✔ forcats 0.5.2 \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: sysfonts\n",
      "\n",
      "R[write to console]: Loading required package: showtextdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(tidyverse)\n",
    "library(ggforce)\n",
    "library(ggpubr)\n",
    "library(showtext)\n",
    "library(rmcorr)\n",
    "library(patchwork)\n",
    "\n",
    "THEME_DEFAULT <- theme_bw(\n",
    "    base_size=10\n",
    ") + theme(\n",
    "        axis.title.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.title.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.text.x=element_text(colour='grey20', size=10),\n",
    "        axis.text.y=element_text(colour='grey20', size=10),\n",
    "        strip.text.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        strip.text.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.background=element_blank(),\n",
    "        legend.title=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.text=element_text(colour='grey20', size=10),\n",
    "        legend.position='top',\n",
    "        legend.box.spacing= unit(0, 'cm'),\n",
    "        plot.subtitle=element_text(colour='grey20', size=10, hjust=.5)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iclab1209/miniconda3/envs/sci-data/lib/python3.9/site-packages/rpy2/robjects/pandas2ri.py:65: UserWarning: Error while trying to convert the column \"feature\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'numpy.str_'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 32 -u cm\n",
    "\n",
    "plots <- list()\n",
    "\n",
    "#for (l in c('valence', 'arousal', 'stress', 'disturbance')) {\n",
    "for (l in c( 'stress')) {\n",
    "    data <- IMPORTANCE_SUMMARY %>% filter(\n",
    "        (label == l)\n",
    "    )\n",
    "\n",
    "    p_label <- ggplot() + geom_text(\n",
    "        aes(x=.5, y=.5),\n",
    "        label=str_to_title(l), \n",
    "        family='ssp', \n",
    "        fontface='bold',\n",
    "        size=4\n",
    "    ) + theme_void()\n",
    "\n",
    "    p_rf <- ggplot(\n",
    "        data %>% filter(alg == 'rf_os') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1, vjust=.5),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='Random Forest'\n",
    "    )\n",
    "    \n",
    "    p_xgb <- ggplot(\n",
    "        data %>% filter(alg == 'xgb_os') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1, vjust=.5),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='XGBoost'\n",
    "    )\n",
    "    \n",
    "    plots[[paste(l, 'label', sep='_')]] <- p_label\n",
    "    plots[[paste(l, 'rf', sep='_')]] <- p_rf\n",
    "    plots[[paste(l, 'xgb', sep='_')]] <- p_xgb\n",
    "}\n",
    "\n",
    "#p <- plots$arousal_label + plots$valence_label\n",
    "#p <- p / (plots$arousal_rf | plots$arousal_xgb | plots$valence_rf | plots$valence_xgb)\n",
    "#p <- p / (plots$stress_label + plots$disturbance_label)\n",
    "#p <- p / (plots$stress_rf | plots$stress_xgb | plots$disturbance_rf | plots$disturbance_xgb)\n",
    "p <- plots$stress_label \n",
    "p <- p / (plots$stress_rf | plots$stress_xgb)\n",
    "\n",
    "p <- p + plot_layout(\n",
    "    heights=c(1.1, 10, 1.1, 10)\n",
    ")\n",
    "\n",
    "ggsave(paste('./fig/imp.pdf'), plot=p, width=26, height=32, unit='cm', device=cairo_pdf)\n",
    "print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
