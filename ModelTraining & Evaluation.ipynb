{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = GroupKFold(n_splits=n_splits \n",
    ")\n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 22:13:59,034\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:00] Success: [dummy#2] Normalizing numeric features (0.25s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:00] Success: [dummy#2] Training (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:00] Success: [dummy#1] Normalizing numeric features (0.25s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:00] Success: [dummy#1] Training (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:00] Success: [dummy#4] Normalizing numeric features (0.27s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:00] Success: [dummy#4] Training (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:00] Success: [dummy#3] Normalizing numeric features (0.27s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:00] Success: [dummy#3] Training (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:00] Success: [dummy#5] Normalizing numeric features (0.23s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:00] In progress: [dummy#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:00] Success: [dummy#5] Training (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:01] Success: [rf_ns#2] Normalizing numeric features (0.28s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:01] Success: [rf_ns#1] Normalizing numeric features (0.24s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:01] Success: [rf_ns#4] Normalizing numeric features (0.29s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:01] Success: [rf_ns#3] Normalizing numeric features (0.28s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:01] Success: [rf_ns#5] Normalizing numeric features (0.27s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:01] In progress: [rf_ns#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:02] Success: [rf_ns#4] 1-th Feature selection (0.93s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Sel. Feat.: 214 (# Cat. = 2; # Num. = 212)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:02] In progress: [rf_ns#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:02] Success: [rf_ns#2] 1-th Feature selection (1.06s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Sel. Feat.: 215 (# Cat. = 2; # Num. = 213)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:02] In progress: [rf_ns#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:02] Success: [rf_ns#5] 1-th Feature selection (1.07s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Sel. Feat.: 223 (# Cat. = 1; # Num. = 222)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:02] In progress: [rf_ns#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:02] Success: [rf_ns#3] 1-th Feature selection (1.18s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Sel. Feat.: 241 (# Cat. = 1; # Num. = 240)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:02] In progress: [rf_ns#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:02] Success: [rf_ns#1] 1-th Feature selection (1.19s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Sel. Feat.: 230 (# Cat. = 2; # Num. = 228)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:02] In progress: [rf_ns#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:03] Success: [rf_ns#4] Training (0.70s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:03] Success: [rf_ns#2] Training (0.74s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:03] Success: [rf_ns#5] Training (0.70s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:03] Success: [rf_ns#1] Training (0.73s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:03] Success: [rf_ns#3] Training (0.73s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:03] Success: [rf_os#1] Normalizing numeric features (0.22s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:03] Success: [rf_os#2] Normalizing numeric features (0.30s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:04] Success: [rf_os#4] Normalizing numeric features (0.28s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:04] In progress: [rf_os#4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:03] Success: [rf_os#3] Normalizing numeric features (0.31s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:03] In progress: [rf_os#3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:04] Success: [rf_os#5] Normalizing numeric features (0.27s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:04] In progress: [rf_os#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:04] Success: [rf_os#4] 1-th Feature selection (0.93s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Sel. Feat.: 214 (# Cat. = 2; # Num. = 212)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:04] In progress: [rf_os#4] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:04] Success: [rf_os#2] 1-th Feature selection (1.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Sel. Feat.: 215 (# Cat. = 2; # Num. = 213)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:04] In progress: [rf_os#2] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:05] Success: [rf_os#1] 1-th Feature selection (1.24s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Sel. Feat.: 230 (# Cat. = 2; # Num. = 228)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:05] In progress: [rf_os#1] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:05] Success: [rf_os#5] 1-th Feature selection (1.19s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Sel. Feat.: 223 (# Cat. = 1; # Num. = 222)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:05] In progress: [rf_os#5] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:05] Success: [rf_os#3] 1-th Feature selection (1.26s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Sel. Feat.: 241 (# Cat. = 1; # Num. = 240)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:05] In progress: [rf_os#3] Oversampling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:30] Success: [rf_os#4] Oversampling (25.85s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:30] In progress: [rf_os#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:30] Success: [rf_os#2] Oversampling (25.99s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:30] In progress: [rf_os#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:31] Success: [rf_os#4] Training (0.72s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:31] Success: [rf_os#2] Training (0.73s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:32] Success: [rf_os#5] Oversampling (27.72s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:32] In progress: [rf_os#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:33] Success: [rf_os#5] Training (0.71s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:34] Success: [rf_os#1] Oversampling (29.25s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:34] In progress: [rf_os#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:35] Success: [rf_os#1] Training (0.75s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:37] Success: [rf_os#3] Oversampling (31.89s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:37] In progress: [rf_os#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:37] Success: [rf_os#3] Training (0.69s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:38] Success: [xgb_ns#1] Normalizing numeric features (0.26s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:38] Success: [xgb_ns#3] Normalizing numeric features (0.30s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:38] Success: [xgb_ns#2] Normalizing numeric features (0.28s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:38] Success: [xgb_ns#4] Normalizing numeric features (0.31s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:38] Success: [xgb_ns#5] Normalizing numeric features (0.30s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:38] In progress: [xgb_ns#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#4] 1-th Feature selection (0.96s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Sel. Feat.: 214 (# Cat. = 2; # Num. = 212)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:39] In progress: [xgb_ns#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#2] 1-th Feature selection (1.11s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Sel. Feat.: 215 (# Cat. = 2; # Num. = 213)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:39] In progress: [xgb_ns#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#1] 1-th Feature selection (1.23s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Sel. Feat.: 230 (# Cat. = 2; # Num. = 228)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:39] In progress: [xgb_ns#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#3] 1-th Feature selection (1.24s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Sel. Feat.: 241 (# Cat. = 1; # Num. = 240)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:39] In progress: [xgb_ns#3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#5] 1-th Feature selection (1.11s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Sel. Feat.: 223 (# Cat. = 1; # Num. = 222)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:39] In progress: [xgb_ns#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#4] Training (0.36s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#2] Training (0.44s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#1] Training (0.37s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:39] Success: [xgb_ns#5] Training (0.33s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:40] Success: [xgb_ns#3] Training (0.50s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:40] Success: [xgb_os#1] Normalizing numeric features (0.24s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:40] Success: [xgb_os#2] Normalizing numeric features (0.29s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:40] Success: [xgb_os#4] Normalizing numeric features (0.29s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:40] Success: [xgb_os#3] Normalizing numeric features (0.28s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:40] Success: [xgb_os#5] Normalizing numeric features (0.28s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:40] In progress: [xgb_os#5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:41] Success: [xgb_os#2] 1-th Feature selection (1.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m - # Sel. Feat.: 215 (# Cat. = 2; # Num. = 213)\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:14:41] In progress: [xgb_os#2] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:41] Success: [xgb_os#4] 1-th Feature selection (0.94s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m - # Sel. Feat.: 214 (# Cat. = 2; # Num. = 212)\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:14:41] In progress: [xgb_os#4] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:41] Success: [xgb_os#1] 1-th Feature selection (1.25s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m - # Sel. Feat.: 230 (# Cat. = 2; # Num. = 228)\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:14:41] In progress: [xgb_os#1] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:42] Success: [xgb_os#5] 1-th Feature selection (1.11s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m - # Sel. Feat.: 223 (# Cat. = 1; # Num. = 222)\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:14:42] In progress: [xgb_os#5] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:42] Success: [xgb_os#3] 1-th Feature selection (1.24s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Orig. Feat.: 5674 (# Cat. = 58; # Num. = 5616)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m - # Sel. Feat.: 241 (# Cat. = 1; # Num. = 240)\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:14:42] In progress: [xgb_os#3] Oversampling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:15:07] Success: [xgb_os#2] Oversampling (25.16s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:15:07] In progress: [xgb_os#2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:15:06] Success: [xgb_os#4] Oversampling (25.16s).\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:15:06] In progress: [xgb_os#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143154)\u001b[0m [23-05-17 22:15:07] Success: [xgb_os#4] Training (0.31s).\n",
      "\u001b[2m\u001b[36m(_train pid=143153)\u001b[0m [23-05-17 22:15:07] Success: [xgb_os#2] Training (0.48s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:15:09] Success: [xgb_os#5] Oversampling (27.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:15:09] In progress: [xgb_os#5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143150)\u001b[0m [23-05-17 22:15:09] Success: [xgb_os#5] Training (0.36s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:15:10] Success: [xgb_os#1] Oversampling (28.65s).\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:15:10] In progress: [xgb_os#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=143152)\u001b[0m [23-05-17 22:15:10] Success: [xgb_os#1] Training (0.35s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:15:12] Success: [xgb_os#3] Oversampling (30.82s).\n",
      "\u001b[2m\u001b[36m(_train pid=143151)\u001b[0m [23-05-17 22:15:12] In progress: [xgb_os#3] Training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    threshold=1e-5\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_DUMMY),\n",
    "        oversample=False,\n",
    "        select=None,\n",
    "        name='dummy'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    )\n",
    "]\n",
    "\n",
    "with on_ray(num_cpus=6):\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):\n",
    "        p = os.path.join(PATH_INTERMEDIATE, 'feat',f'{l}.pkl')\n",
    "        par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', l)\n",
    "        os.makedirs(par_dir, exist_ok=True)\n",
    "        \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        \n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] \n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([feat_time, feat_current_sensor],axis=1)\n",
    "        feat_final = pd.concat([feat_baseline, feat_yesterday_sensor, feat_today_sensor],axis=1)\n",
    "        X=feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups,\n",
    "            path=par_dir,\n",
    "            categories=cats,\n",
    "            normalize=True,\n",
    "            split='kfold',\n",
    "            split_params= {'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_prob,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            ) if not is_same_y else np.nan\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_prob[:, 1], average=None\n",
    "        ) if not is_same_y else np.nan\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_prob[:, i], pos_label=c, average=None\n",
    "            ) \n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_prob, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_prob=y_prob[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>split</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_class_ratio</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>...</th>\n",
       "      <th>train_pre_1</th>\n",
       "      <th>train_rec_1</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_roauc</th>\n",
       "      <th>train_prauc_0</th>\n",
       "      <th>train_prauc_ref_0</th>\n",
       "      <th>train_prauc_1</th>\n",
       "      <th>train_prauc_ref_1</th>\n",
       "      <th>train_log_loss</th>\n",
       "      <th>train_brier_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>524</td>\n",
       "      <td>253</td>\n",
       "      <td>271</td>\n",
       "      <td>0.933579</td>\n",
       "      <td>177</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845331</td>\n",
       "      <td>0.801661</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.904460</td>\n",
       "      <td>0.889578</td>\n",
       "      <td>0.482578</td>\n",
       "      <td>0.914202</td>\n",
       "      <td>0.517422</td>\n",
       "      <td>0.451943</td>\n",
       "      <td>0.141626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>3</td>\n",
       "      <td>241</td>\n",
       "      <td>524</td>\n",
       "      <td>253</td>\n",
       "      <td>271</td>\n",
       "      <td>0.933579</td>\n",
       "      <td>163</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846920</td>\n",
       "      <td>0.862546</td>\n",
       "      <td>0.854662</td>\n",
       "      <td>0.926609</td>\n",
       "      <td>0.920127</td>\n",
       "      <td>0.482578</td>\n",
       "      <td>0.927720</td>\n",
       "      <td>0.517422</td>\n",
       "      <td>0.397754</td>\n",
       "      <td>0.120471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>rf_os</td>\n",
       "      <td>3</td>\n",
       "      <td>241</td>\n",
       "      <td>524</td>\n",
       "      <td>253</td>\n",
       "      <td>271</td>\n",
       "      <td>0.933579</td>\n",
       "      <td>179</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.152106</td>\n",
       "      <td>0.028671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>2</td>\n",
       "      <td>215</td>\n",
       "      <td>524</td>\n",
       "      <td>253</td>\n",
       "      <td>271</td>\n",
       "      <td>0.933579</td>\n",
       "      <td>159</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849908</td>\n",
       "      <td>0.851476</td>\n",
       "      <td>0.850691</td>\n",
       "      <td>0.924628</td>\n",
       "      <td>0.913816</td>\n",
       "      <td>0.482578</td>\n",
       "      <td>0.930476</td>\n",
       "      <td>0.517422</td>\n",
       "      <td>0.405632</td>\n",
       "      <td>0.123919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>xgb_os</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>523</td>\n",
       "      <td>252</td>\n",
       "      <td>271</td>\n",
       "      <td>0.929889</td>\n",
       "      <td>189</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.787823</td>\n",
       "      <td>0.827519</td>\n",
       "      <td>0.912165</td>\n",
       "      <td>0.901635</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.914625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434818</td>\n",
       "      <td>0.133941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label     alg split  n_feature  test_inst  test_inst_0  test_inst_1  \\\n",
       "0  stress  xgb_ns     1        230        524          253          271   \n",
       "1  stress  xgb_ns     3        241        524          253          271   \n",
       "2  stress   rf_os     3        241        524          253          271   \n",
       "3  stress  xgb_ns     2        215        524          253          271   \n",
       "4  stress  xgb_os     5        223        523          252          271   \n",
       "\n",
       "   test_class_ratio  test_true_0_pred_0  test_true_0_pred_1  ...  train_pre_1  \\\n",
       "0          0.933579                 177                  76  ...     0.845331   \n",
       "1          0.933579                 163                  90  ...     0.846920   \n",
       "2          0.933579                 179                  74  ...     0.999077   \n",
       "3          0.933579                 159                  94  ...     0.849908   \n",
       "4          0.929889                 189                  63  ...     0.871429   \n",
       "\n",
       "   train_rec_1  train_f1_1  train_roauc  train_prauc_0  train_prauc_ref_0  \\\n",
       "0     0.801661    0.822917     0.904460       0.889578           0.482578   \n",
       "1     0.862546    0.854662     0.926609       0.920127           0.482578   \n",
       "2     0.999077    0.999077     0.999977       0.999975           0.500000   \n",
       "3     0.851476    0.850691     0.924628       0.913816           0.482578   \n",
       "4     0.787823    0.827519     0.912165       0.901635           0.500000   \n",
       "\n",
       "   train_prauc_1  train_prauc_ref_1  train_log_loss  train_brier_loss  \n",
       "0       0.914202           0.517422        0.451943          0.141626  \n",
       "1       0.927720           0.517422        0.397754          0.120471  \n",
       "2       0.999976           0.500000        0.152106          0.028671  \n",
       "3       0.930476           0.517422        0.405632          0.123919  \n",
       "4       0.914625           0.500000        0.434818          0.133941  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "#for l in ['valence', 'arousal', 'disturbance', 'stress']:\n",
    "for l in [ 'stress']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        if f == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        model, pid = f[:f.index('.pkl')].split('#')\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "        X, y = res.X_test, res.y_test\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_test = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        X, y = res.X_train, res.y_train\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_prob = res.estimator.predict_proba(X)\n",
    "        ev_train = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_prob,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        RESULTS_EVAL.append({\n",
    "            'label': l,\n",
    "            'alg': model,\n",
    "            'split': pid,\n",
    "            'n_feature': len(X.columns),\n",
    "            **{\n",
    "                f'test_{k}': v for k, v in ev_test.items()\n",
    "            },\n",
    "            **{\n",
    "                f'train_{k}': v for k, v in ev_train.items()\n",
    "            }\n",
    "        })\n",
    "    \n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>metric</th>\n",
       "      <th>n</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>value_count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>med</th>\n",
       "      <th>range</th>\n",
       "      <th>conf.</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>split</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2:1, 3:1, 5:1, 4:1, 1:1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>n_feature</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28370.0</td>\n",
       "      <td>5674.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5674.0</td>\n",
       "      <td>(5674, 5674)</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>524.0</td>\n",
       "      <td>(523, 524)</td>\n",
       "      <td>(523.2447109789604, 524.3552890210395)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst_0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>252.8</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>253.0</td>\n",
       "      <td>(252, 253)</td>\n",
       "      <td>(252.24471097896046, 253.35528902103957)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stress</td>\n",
       "      <td>dummy</td>\n",
       "      <td>test_inst_1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>271.0</td>\n",
       "      <td>(271, 271)</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label    alg       metric  n  cardinality              value_count  \\\n",
       "0  stress  dummy        split  5          5.0  2:1, 3:1, 5:1, 4:1, 1:1   \n",
       "1  stress  dummy    n_feature  5          NaN                      NaN   \n",
       "2  stress  dummy    test_inst  5          NaN                      NaN   \n",
       "3  stress  dummy  test_inst_0  5          NaN                      NaN   \n",
       "4  stress  dummy  test_inst_1  5          NaN                      NaN   \n",
       "\n",
       "       sum    mean        SD     med         range  \\\n",
       "0      NaN     NaN       NaN     NaN           NaN   \n",
       "1  28370.0  5674.0  0.000000  5674.0  (5674, 5674)   \n",
       "2   2619.0   523.8  0.447214   524.0    (523, 524)   \n",
       "3   1264.0   252.8  0.447214   253.0    (252, 253)   \n",
       "4   1355.0   271.0  0.000000   271.0    (271, 271)   \n",
       "\n",
       "                                      conf.  nan_count  \n",
       "0                                       NaN        NaN  \n",
       "1                                (nan, nan)        0.0  \n",
       "2    (523.2447109789604, 524.3552890210395)        0.0  \n",
       "3  (252.24471097896046, 253.35528902103957)        0.0  \n",
       "4                                (nan, nan)        0.0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "    ['label', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                alg=row.alg,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"13\" halign=\"left\">mean_sd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_f1_0</th>\n",
       "      <th>test_f1_1</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>train_class_ratio</th>\n",
       "      <th>train_f1_0</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_f1_macro</th>\n",
       "      <th>train_inst_0</th>\n",
       "      <th>train_inst_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">stress</th>\n",
       "      <th>dummy</th>\n",
       "      <td>5674.0 (0.0)</td>\n",
       "      <td>0.517 (0.0)</td>\n",
       "      <td>0.0 (0.0)</td>\n",
       "      <td>0.682 (0.0)</td>\n",
       "      <td>0.341 (0.0)</td>\n",
       "      <td>252.8 (0.447)</td>\n",
       "      <td>271.0 (0.0)</td>\n",
       "      <td>0.933 (0.0)</td>\n",
       "      <td>0.0 (0.0)</td>\n",
       "      <td>0.682 (0.0)</td>\n",
       "      <td>0.341 (0.0)</td>\n",
       "      <td>1011.2 (0.447)</td>\n",
       "      <td>1084.0 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_ns</th>\n",
       "      <td>224.6 (11.238)</td>\n",
       "      <td>0.69 (0.02)</td>\n",
       "      <td>0.683 (0.023)</td>\n",
       "      <td>0.697 (0.019)</td>\n",
       "      <td>0.69 (0.02)</td>\n",
       "      <td>252.8 (0.447)</td>\n",
       "      <td>271.0 (0.0)</td>\n",
       "      <td>0.933 (0.0)</td>\n",
       "      <td>0.998 (0.0)</td>\n",
       "      <td>0.999 (0.0)</td>\n",
       "      <td>0.998 (0.0)</td>\n",
       "      <td>1011.2 (0.447)</td>\n",
       "      <td>1084.0 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_os</th>\n",
       "      <td>224.6 (11.238)</td>\n",
       "      <td>0.693 (0.021)</td>\n",
       "      <td>0.685 (0.024)</td>\n",
       "      <td>0.7 (0.021)</td>\n",
       "      <td>0.693 (0.021)</td>\n",
       "      <td>252.8 (0.447)</td>\n",
       "      <td>271.0 (0.0)</td>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.998 (0.0)</td>\n",
       "      <td>0.998 (0.0)</td>\n",
       "      <td>0.998 (0.0)</td>\n",
       "      <td>1084.0 (0.0)</td>\n",
       "      <td>1084.0 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_ns</th>\n",
       "      <td>224.6 (11.238)</td>\n",
       "      <td>0.664 (0.027)</td>\n",
       "      <td>0.65 (0.035)</td>\n",
       "      <td>0.677 (0.021)</td>\n",
       "      <td>0.664 (0.027)</td>\n",
       "      <td>252.8 (0.447)</td>\n",
       "      <td>271.0 (0.0)</td>\n",
       "      <td>0.933 (0.0)</td>\n",
       "      <td>0.826 (0.015)</td>\n",
       "      <td>0.836 (0.017)</td>\n",
       "      <td>0.831 (0.016)</td>\n",
       "      <td>1011.2 (0.447)</td>\n",
       "      <td>1084.0 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_os</th>\n",
       "      <td>224.6 (11.238)</td>\n",
       "      <td>0.681 (0.013)</td>\n",
       "      <td>0.678 (0.018)</td>\n",
       "      <td>0.684 (0.022)</td>\n",
       "      <td>0.681 (0.014)</td>\n",
       "      <td>252.8 (0.447)</td>\n",
       "      <td>271.0 (0.0)</td>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.844 (0.024)</td>\n",
       "      <td>0.833 (0.026)</td>\n",
       "      <td>0.839 (0.025)</td>\n",
       "      <td>1084.0 (0.0)</td>\n",
       "      <td>1084.0 (0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean_sd                                               \\\n",
       "metric              n_feature       test_acc      test_f1_0      test_f1_1   \n",
       "label  alg                                                                   \n",
       "stress dummy     5674.0 (0.0)    0.517 (0.0)      0.0 (0.0)    0.682 (0.0)   \n",
       "       rf_ns   224.6 (11.238)    0.69 (0.02)  0.683 (0.023)  0.697 (0.019)   \n",
       "       rf_os   224.6 (11.238)  0.693 (0.021)  0.685 (0.024)    0.7 (0.021)   \n",
       "       xgb_ns  224.6 (11.238)  0.664 (0.027)   0.65 (0.035)  0.677 (0.021)   \n",
       "       xgb_os  224.6 (11.238)  0.681 (0.013)  0.678 (0.018)  0.684 (0.022)   \n",
       "\n",
       "                                                                            \\\n",
       "metric         test_f1_macro    test_inst_0  test_inst_1 train_class_ratio   \n",
       "label  alg                                                                   \n",
       "stress dummy     0.341 (0.0)  252.8 (0.447)  271.0 (0.0)       0.933 (0.0)   \n",
       "       rf_ns     0.69 (0.02)  252.8 (0.447)  271.0 (0.0)       0.933 (0.0)   \n",
       "       rf_os   0.693 (0.021)  252.8 (0.447)  271.0 (0.0)         1.0 (0.0)   \n",
       "       xgb_ns  0.664 (0.027)  252.8 (0.447)  271.0 (0.0)       0.933 (0.0)   \n",
       "       xgb_os  0.681 (0.014)  252.8 (0.447)  271.0 (0.0)         1.0 (0.0)   \n",
       "\n",
       "                                                                            \\\n",
       "metric            train_f1_0     train_f1_1 train_f1_macro    train_inst_0   \n",
       "label  alg                                                                   \n",
       "stress dummy       0.0 (0.0)    0.682 (0.0)    0.341 (0.0)  1011.2 (0.447)   \n",
       "       rf_ns     0.998 (0.0)    0.999 (0.0)    0.998 (0.0)  1011.2 (0.447)   \n",
       "       rf_os     0.998 (0.0)    0.998 (0.0)    0.998 (0.0)    1084.0 (0.0)   \n",
       "       xgb_ns  0.826 (0.015)  0.836 (0.017)  0.831 (0.016)  1011.2 (0.447)   \n",
       "       xgb_os  0.844 (0.024)  0.833 (0.026)  0.839 (0.025)    1084.0 (0.0)   \n",
       "\n",
       "                             \n",
       "metric         train_inst_1  \n",
       "label  alg                   \n",
       "stress dummy   1084.0 (0.0)  \n",
       "       rf_ns   1084.0 (0.0)  \n",
       "       rf_os   1084.0 (0.0)  \n",
       "       xgb_ns  1084.0 (0.0)  \n",
       "       xgb_os  1084.0 (0.0)  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "        ['n_feature', 'train_class_ratio', 'train_inst_0', 'train_inst_1', 'test_inst_0', 'test_inst_1', 'test_acc', 'test_f1_0' ,'test_f1_1', 'test_f1_macro', 'train_f1_0' ,'train_f1_1', 'train_f1_macro',]\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep='')\n",
    ").pivot(\n",
    "    index=['label', 'alg'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if hasattr(estimator, 'model'):\n",
    "        estimator = estimator.model\n",
    "    \n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "\n",
    "for f in os.listdir(DIR_EVAL):\n",
    "    res = load(os.path.join(DIR_EVAL, f))\n",
    "    \n",
    "    f_norm = f[:f.index('.pkl')]\n",
    "    cv = f_norm[:f.rindex('#')]\n",
    "    \n",
    "    feat_imp = feature_importance(res.estimator)\n",
    "    if not feat_imp:\n",
    "        continue\n",
    "        \n",
    "    names, importance = feat_imp\n",
    "    d = pd.DataFrame(\n",
    "        importance.reshape(1, -1),\n",
    "        columns=names\n",
    "    )\n",
    "    IMPORTANCE_EVAL[cv].append(d)\n",
    "\n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for k, v in IMPORTANCE_EVAL.items():\n",
    "    l, o, a = k.split('#')\n",
    "    \n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        oversampling=o,\n",
    "        alg=a\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0)\n",
    "IMPORTANCE_SUMMARY.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 22 -u cm\n",
    "\n",
    "plots <- list()\n",
    "\n",
    "#for (l in c('valence', 'arousal', 'stress', 'disturbance')) {\n",
    "for (l in c( 'stress')) {\n",
    "    data <- IMPORTANCE_SUMMARY %>% filter(\n",
    "        (label == l) & (oversampling == 'os')\n",
    "    )\n",
    "\n",
    "    p_label <- ggplot() + geom_text(\n",
    "        aes(x=.5, y=.5),\n",
    "        label=str_to_title(l), \n",
    "        family='ssp', \n",
    "        fontface='bold',\n",
    "        size=4\n",
    "    ) + theme_void()\n",
    "\n",
    "    data_rf <- data %>% filter(\n",
    "        alg == 'rf'\n",
    "    )\n",
    "\n",
    "    p_rf <- ggplot(\n",
    "        data %>% filter(alg == 'rf') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='Random Forest'\n",
    "    ) + ylim(\n",
    "        c(0, 0.08)\n",
    "    )\n",
    "    p_xgb <- ggplot(\n",
    "        data %>% filter(alg == 'xgb') %>% top_n(n=10, wt=importance),\n",
    "        aes(x=reorder(feature, -importance), y=importance),\n",
    "    ) + geom_col(\n",
    "    ) + THEME_DEFAULT + theme(\n",
    "        axis.text.x=element_text(angle=90, size=10, hjust=1),\n",
    "        axis.title.x=element_blank(),\n",
    "        axis.title.y=element_blank()\n",
    "    ) + labs(\n",
    "        subtitle='XGBoost'\n",
    "    ) + ylim(\n",
    "        c(0, 0.08)\n",
    "    )\n",
    "    plots[[paste(l, 'label', sep='_')]] <- p_label\n",
    "    plots[[paste(l, 'rf', sep='_')]] <- p_rf\n",
    "    plots[[paste(l, 'xgb', sep='_')]] <- p_xgb\n",
    "}\n",
    "\n",
    "#p <- plots$arousal_label + plots$valence_label\n",
    "#p <- p / (plots$arousal_rf | plots$arousal_xgb | plots$valence_rf | plots$valence_xgb)\n",
    "#p <- p / (plots$stress_label + plots$disturbance_label)\n",
    "#p <- p / (plots$stress_rf | plots$stress_xgb | plots$disturbance_rf | plots$disturbance_xgb)\n",
    "p <- plots$stress_label \n",
    "p <- p / (plots$stress_rf | plots$stress_xgb)\n",
    "\n",
    "\n",
    "p <- p + plot_layout(\n",
    "    heights=c(1.1, 10, 1.1, 10)\n",
    ")\n",
    "\n",
    "ggsave(paste('./fig/imp.pdf'), plot=p, width=26, height=20, unit='cm', device=cairo_pdf)\n",
    "print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
