{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress-fixed.pkl')\n",
    "\n",
    "X, y, groups, t, datetimes = load(p)\n",
    "##############################################\n",
    "# #Remove users with extreme label distribution\n",
    "# # Create a DataFrame from y, groups, t, datetimes\n",
    "# info_df = pd.DataFrame({\n",
    "#     'y': y,\n",
    "#     'groups': groups,\n",
    "#     't': t,\n",
    "#     'datetimes': pd.to_datetime(datetimes)  # assuming 'datetimes' needs conversion to datetime\n",
    "# })\n",
    "\n",
    "# # Calculate majority/minority ratio for each group\n",
    "# def calculate_ratio(group):\n",
    "#     counts = group['y'].value_counts()\n",
    "#     if len(counts) > 1:\n",
    "#         majority = counts.max()\n",
    "#         minority = counts.min()\n",
    "#         ratio = majority / minority\n",
    "#     else:\n",
    "#         ratio = np.inf  # Infinite ratio if there's no minority class\n",
    "#     return ratio\n",
    "\n",
    "# # Apply the function per group\n",
    "# group_ratios = info_df.groupby('groups').apply(calculate_ratio)\n",
    "\n",
    "# # Filter groups based on the ratio\n",
    "# filtered_groups = group_ratios[group_ratios <= 4].index\n",
    "\n",
    "# # Filter the original DataFrame 'info_df' to remove skewed groups\n",
    "# filtered_info = info_df[info_df['groups'].isin(filtered_groups)]\n",
    "\n",
    "# # Use the indices of the filtered info to refine 'X'\n",
    "# X_filtered = X.loc[filtered_info.index]\n",
    "\n",
    "# # Extracting other arrays from the filtered info\n",
    "# y_filtered = filtered_info['y'].values\n",
    "# groups_filtered = filtered_info['groups'].values\n",
    "# t_filtered = filtered_info['t'].values\n",
    "# datetimes_filtered = filtered_info['datetimes'].values\n",
    "\n",
    "# X, y, groups, t, datetimes = X_filtered, y_filtered, groups_filtered, t_filtered, datetimes_filtered\n",
    "\n",
    "# Now 'X_filtered', 'y_filtered', 'groups_filtered', 't_filtered', 'datetimes_filtered'\n",
    "# are ready to be used for further analysis or modeling\n",
    "################################################\n",
    "# #Remove neutral state samples\n",
    "# y =  LABELS_PROC['stressLevel'].to_numpy()\n",
    "\n",
    "# # Create a mask that selects all samples where y is not equal to 3 (neutral state)\n",
    "# mask = y != 3\n",
    "\n",
    "# # Apply this mask to filter out the neutral samples from all arrays\n",
    "# X_filtered = X[mask]  # X is a DataFrame, it uses boolean indexing directly\n",
    "# y_filtered = y[mask]  # y, groups, t, datetimes are numpy arrays or similar structures\n",
    "# groups_filtered = groups[mask]\n",
    "# t_filtered = t[mask]\n",
    "# datetimes_filtered = datetimes[mask]\n",
    "\n",
    "# y = (y_filtered > 3).astype(int)\n",
    "# X = X_filtered\n",
    "# groups = groups_filtered\n",
    "# t = t_filtered\n",
    "# datetimes = datetimes_filtered\n",
    "\n",
    "################################################\n",
    "#Use mean threshold for all users (only training set,\\ \n",
    "#we need to use raw value and binarize after data splitting)\n",
    "# y =  LABELS_PROC['stressLevel'].to_numpy()\n",
    "#Use user speicifc mean threshold\n",
    "# y =LABELS_PROC['stress_user_mean'].to_numpy()\n",
    "#Use fixed threshold\n",
    "#         y =LABELS_PROC['stress_fixed'].to_numpy()\n",
    "#Use three categories (fixed threshold) \n",
    "#        y =LABELS_PROC['stress_fixed_tri'].to_numpy()\n",
    "\n",
    "\n",
    "#The following code is designed for reordering for the sake of time series split \n",
    "#################################################\n",
    "# Create a DataFrame with user_id and datetime\n",
    "\n",
    "df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
    "\n",
    "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "\n",
    "# Normalize the datetime for each user only needed for timeseries split/groupk partil personalization\n",
    "#         df_merged['datetime'] = df_merged.groupby('user_id')['datetime'].transform(lambda x: x - x.min())\n",
    "# df_merged['datetime'] = df_merged.groupby('user_id')['datetime'].transform(lambda x: x - x.min().normalize())\n",
    "\n",
    "# Sort the DataFrame by datetime\n",
    "df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
    "# df_merged = df_merged.sort_values(by=['datetime'])\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "# df_merged = df_merged.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "# Update groups and datetimes\n",
    "groups = df_merged['user_id'].to_numpy()\n",
    "datetimes = df_merged['datetime'].to_numpy()  \n",
    "y = df_merged['label'].to_numpy()\n",
    "X = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
    "\n",
    "#The following code is for shuffling the temporal order for all users\n",
    "########################################################\n",
    "\n",
    "# # Assuming 'groups', 'datetimes', 'y', and 'X' are already defined and loaded\n",
    "# # Create a DataFrame with user_id, datetime, and label\n",
    "# df = pd.DataFrame({\n",
    "#     'user_id': groups,\n",
    "#     'datetime': datetimes,\n",
    "#     'label': y\n",
    "# })\n",
    "\n",
    "# # Merge the new DataFrame with the features DataFrame 'X'\n",
    "# # Ensure 'X' is indexed the same way as 'groups', 'datetimes', and 'y'\n",
    "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "\n",
    "# # Shuffle the DataFrame\n",
    "# # This disregards the temporal ordering completely and randomizes all entries\n",
    "# df_merged = df_merged.sample(frac=1, random_state=42)  # Use a fixed seed for reproducibility\n",
    "\n",
    "# # Extract the shuffled 'groups', 'datetimes', 'y', and 'X' from the shuffled DataFrame\n",
    "# groups_shuffled = df_merged['user_id'].to_numpy()\n",
    "# datetimes_shuffled = df_merged['datetime'].to_numpy()\n",
    "# y_shuffled = df_merged['label'].to_numpy()\n",
    "# X_shuffled = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
    "\n",
    "# # Optionally, you can convert 'X_shuffled' back to the correct type if it needs to be a DataFrame\n",
    "# X_shuffled = pd.DataFrame(X_shuffled, columns=X.columns)\n",
    "\n",
    "# X, y, groups, datetimes = X_shuffled, y_shuffled, groups_shuffled, datetimes_shuffled\n",
    "\n",
    "\n",
    "#The following code is for only using 1st day\n",
    "###########################################\n",
    "# filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "# # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_week.csv'),index_col=0)\n",
    "# X_filtered = X[~X.index.isin(filtered_df.index)]\n",
    "# y_series = pd.Series(y, index=X.index)\n",
    "# y_filtered = y_series[~y_series.index.isin(filtered_df.index)]\n",
    "# y_filtered = y_filtered.values\n",
    "# groups_series = pd.Series(groups, index=X.index)\n",
    "# groups_filtered = groups_series[~groups_series.index.isin(filtered_df.index)]\n",
    "# groups_filtered = groups_filtered.values\n",
    "# X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "# #The following code is for excluding using 1st day\n",
    "# ###########################################\n",
    "# # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_week.csv'),index_col=0)\n",
    "# filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "# X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "# y_series = pd.Series(y, index=X.index)\n",
    "# y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "# y_filtered = y_filtered.values\n",
    "# groups_series = pd.Series(groups, index=X.index)\n",
    "# groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "# groups_filtered = groups_filtered.values\n",
    "# datetimes_series = pd.Series(datetimes, index=X.index)\n",
    "# datetimes_filtered = datetimes_series[datetimes_series.index.isin(filtered_df.index)]\n",
    "# datetimes_filtered = datetimes_filtered.values\n",
    "# X,y, groups, datetimes=X_filtered,y_filtered, groups_filtered, datetimes_filtered\n",
    "\n",
    "\n",
    "###########################################\n",
    "#The following code is for similar-user model\n",
    "###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "###########################################\n",
    "#Remove low frequency features\n",
    "#         mask = ['CAE#', 'MED#', 'ONF#', 'PWS#', 'RNG#','MSG#' ]\n",
    "#         X = X.loc[:, [all(m not in str(x) for m in mask) for x in X.columns]]\n",
    "\n",
    "#Divide the features into different categories\n",
    "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
    "#Divide the time window features into sensor/past stress label\n",
    "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]\n",
    "\n",
    "\n",
    "\n",
    "#Prepare the final feature set\n",
    "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "#The following code is for calculating aggregated features\n",
    "########################################################################\n",
    "# # Define a function to split the column name into sensor and attribute\n",
    "# def split_column_name(col_name):\n",
    "#     parts = col_name.rsplit(\"#\", 1)  # Split on last occurrence of '#'\n",
    "#     return parts[0]  # This gives you 'Sensor#Attribute'\n",
    "\n",
    "# # Get a list of unique sensor-attribute combinations\n",
    "# df=feat_today_sensor\n",
    "# sensor_attributes = df.columns.map(split_column_name).unique()\n",
    "\n",
    "# # Create a list to hold the aggregated results\n",
    "# agg_results = []\n",
    "\n",
    "# # Loop over each sensor-attribute, select the appropriate columns, compute the mean and std\n",
    "# for sensor_attribute in sensor_attributes:\n",
    "#     # Select columns for this sensor-attribute\n",
    "#     cols_to_aggregate = [col for col in df.columns if col.startswith(sensor_attribute)]\n",
    "#     # Compute the mean and std and store in the new DataFrame\n",
    "#     agg_results.append(df[cols_to_aggregate].mean(axis=1).rename(sensor_attribute + '|'+ 'MEAN'))\n",
    "#     agg_results.append(df[cols_to_aggregate].std(axis=1).rename(sensor_attribute + '|'+'STD'))\n",
    "\n",
    "# # Concatenate all the results into a single DataFrame\n",
    "# agg_feature = pd.concat(agg_results, axis=1)\n",
    "\n",
    "######################################################################\n",
    "feat_final = pd.concat([feat_baseline],axis=1)\n",
    "\n",
    "#         # Fill NaN values with zeros\n",
    "#         feat_final = feat_final.fillna(0)\n",
    "\n",
    "#         # Find the maximum non-infinity value and minimum non-negative infinity value across the entire dataframe\n",
    "#         max_val = feat_final[feat_final != np.inf].max().max()\n",
    "#         min_val = feat_final[feat_final != -np.inf].min().min()\n",
    "\n",
    "#         # Replace positive and negative infinity values\n",
    "#         feat_final.replace(np.inf, max_val, inplace=True)\n",
    "#         feat_final.replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "X = feat_final\n",
    "cats = X.columns[X.dtypes == bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        # After fitting, store the best iteration\n",
    "        # self.best_iteration_ = self.model.get_booster().best_iteration\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "        # return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)\n",
    "        # return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:36:55,503\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fold pid=1110891)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110892)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110900)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110896)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110899)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110891)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110893)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110905)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110904)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110902)\u001b[0m Training completed for Fold_22 with AUC: 0.4838709677419355\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110899)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110894)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110891)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110893)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110896)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110899)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=1110890)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "0.5184646944602751\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import traceback\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import ray\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    metrics: dict\n",
    "    duration: float\n",
    "\n",
    "def log(message: str):\n",
    "    print(message)  # Simple logging to stdout or enhance as needed\n",
    "\n",
    "@ray.remote\n",
    "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if normalize:\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "        \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            \n",
    "        if select:\n",
    "            # # Removing low variance features\n",
    "            # X_train = exclude_low_variance(X_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            # #Removing highly correlated features\n",
    "            # X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            if isinstance(select, SelectFromModel):\n",
    "                select = [select]\n",
    "                \n",
    "            for i, s in enumerate(select):\n",
    "                C = np.asarray(X_train.columns)\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "\n",
    "        if oversample:\n",
    "            if len(C_cat) > 0:\n",
    "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        estimator = clone(estimator).fit(X_train, y_train)\n",
    "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
    "\n",
    "        result = FoldResult(\n",
    "            name=fold_name,\n",
    "            metrics={'AUC': auc_score},\n",
    "            duration=time.time() - start_time\n",
    "        )\n",
    "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
    "        return None\n",
    "\n",
    "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
    "    if not ray.is_initialized():\n",
    "        ray.init()\n",
    "\n",
    "    futures = []\n",
    "    splitter = LeaveOneGroupOut()  # Or any other CV strategy\n",
    "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        C_cat = np.asarray(sorted(cats))\n",
    "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "        job = train_fold.remote('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
    "        futures.append(job)\n",
    "\n",
    "    results = ray.get(futures)\n",
    "    return results\n",
    "\n",
    "with on_ray():\n",
    "    SELECT_LASSO = SelectFromModel(\n",
    "            estimator=LogisticRegression(\n",
    "            penalty='l1' \n",
    "            ,solver='liblinear'\n",
    "            , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "        ),\n",
    "        threshold = 0.005\n",
    "    )\n",
    "    # Example usage\n",
    "    estimator = EvXGBClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        eval_metric='logloss', \n",
    "        eval_size=0.2,\n",
    "        early_stopping_rounds=10, \n",
    "        objective='binary:logistic', \n",
    "        verbosity=0,\n",
    "        learning_rate=0.01,\n",
    "    )  \n",
    "    results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
    "    auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
    "    mean_auc = np.mean(auc_values)\n",
    "    print(mean_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-23 19:11:39</td></tr>\n",
       "<tr><td>Running for: </td><td>01:29:29.43        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.2/62.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using HyperBand: num_stopped=0 total_brackets=3<br>Round #0:<br>  Bracket(Max Size (n)=5, Milestone (r)=81, completed=100.0%): {TERMINATED: 5} <br>  Bracket(Max Size (n)=8, Milestone (r)=27, completed=1.6%): {RUNNING: 2, TERMINATED: 6} <br>  Bracket(Max Size (n)=15, Milestone (r)=9, completed=0.0%): {PENDING: 1, RUNNING: 8} <br>Logical resource usage: 10.0/16 CPUs, 0.9999999999999999/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  colsample_bylevel</th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">   early_stopping_round\n",
       "s</th><th style=\"text-align: right;\">    gamma</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  num_parallel_tree</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  subsample</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">     auc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_9042ea90</td><td>RUNNING   </td><td>143.248.55.56:1112516</td><td style=\"text-align: right;\">           0.737952</td><td style=\"text-align: right;\">          0.51833 </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.30717  </td><td style=\"text-align: right;\">      0.0772977</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  1.63589  </td><td style=\"text-align: right;\">     1.42362</td><td style=\"text-align: right;\">   0.865118</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_e0abbe7c</td><td>RUNNING   </td><td>143.248.55.56:1112661</td><td style=\"text-align: right;\">           0.94299 </td><td style=\"text-align: right;\">          0.97431 </td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.496529 </td><td style=\"text-align: right;\">      0.0856561</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">  1.2759   </td><td style=\"text-align: right;\">     2.06422</td><td style=\"text-align: right;\">   0.931888</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_c5af3588</td><td>RUNNING   </td><td>143.248.55.56:1112774</td><td style=\"text-align: right;\">           0.640563</td><td style=\"text-align: right;\">          0.862017</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.108067 </td><td style=\"text-align: right;\">      0.0310086</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  1.5359   </td><td style=\"text-align: right;\">     1.36514</td><td style=\"text-align: right;\">   0.722772</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_ac060505</td><td>RUNNING   </td><td>143.248.55.56:1112808</td><td style=\"text-align: right;\">           0.649807</td><td style=\"text-align: right;\">          0.547243</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.180454 </td><td style=\"text-align: right;\">      0.036182 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  0.658766 </td><td style=\"text-align: right;\">     1.53665</td><td style=\"text-align: right;\">   0.680137</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_378d2c63</td><td>RUNNING   </td><td>143.248.55.56:1112845</td><td style=\"text-align: right;\">           0.575578</td><td style=\"text-align: right;\">          0.86982 </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.352948 </td><td style=\"text-align: right;\">      0.0219815</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  0.125953 </td><td style=\"text-align: right;\">     2.57225</td><td style=\"text-align: right;\">   0.987061</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_89abdd6f</td><td>RUNNING   </td><td>143.248.55.56:1112876</td><td style=\"text-align: right;\">           0.922522</td><td style=\"text-align: right;\">          0.609291</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.402052 </td><td style=\"text-align: right;\">      0.0570632</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  0.121142 </td><td style=\"text-align: right;\">     4.51464</td><td style=\"text-align: right;\">   0.652335</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_78e8377b</td><td>RUNNING   </td><td>143.248.55.56:1112913</td><td style=\"text-align: right;\">           0.610781</td><td style=\"text-align: right;\">          0.903087</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.432941 </td><td style=\"text-align: right;\">      0.0832338</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  0.885319 </td><td style=\"text-align: right;\">     4.67721</td><td style=\"text-align: right;\">   0.805687</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_3152a840</td><td>RUNNING   </td><td>143.248.55.56:1112963</td><td style=\"text-align: right;\">           0.795658</td><td style=\"text-align: right;\">          0.942984</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.232435 </td><td style=\"text-align: right;\">      0.0949728</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  1.93996  </td><td style=\"text-align: right;\">     4.57137</td><td style=\"text-align: right;\">   0.791174</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_28f3e760</td><td>RUNNING   </td><td>143.248.55.56:1112997</td><td style=\"text-align: right;\">           0.708909</td><td style=\"text-align: right;\">          0.699411</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.295485 </td><td style=\"text-align: right;\">      0.0790746</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  0.427119 </td><td style=\"text-align: right;\">     2.12773</td><td style=\"text-align: right;\">   0.884209</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_0481a670</td><td>RUNNING   </td><td>143.248.55.56:1113028</td><td style=\"text-align: right;\">           0.981912</td><td style=\"text-align: right;\">          0.7089  </td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.121464 </td><td style=\"text-align: right;\">      0.0114447</td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">  0.369108 </td><td style=\"text-align: right;\">     3.50469</td><td style=\"text-align: right;\">   0.610667</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_aa03b477</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">           0.788346</td><td style=\"text-align: right;\">          0.684028</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.23689  </td><td style=\"text-align: right;\">      0.0517673</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">  0.893543 </td><td style=\"text-align: right;\">     3.23049</td><td style=\"text-align: right;\">   0.740344</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>objective_51fea836</td><td>TERMINATED</td><td>143.248.55.56:1112258</td><td style=\"text-align: right;\">           0.591681</td><td style=\"text-align: right;\">          0.674317</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.167184 </td><td style=\"text-align: right;\">      0.0345109</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  0.967465 </td><td style=\"text-align: right;\">     3.37525</td><td style=\"text-align: right;\">   0.74798 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2618.98</td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "<tr><td>objective_c752e991</td><td>TERMINATED</td><td>143.248.55.56:1112289</td><td style=\"text-align: right;\">           0.800188</td><td style=\"text-align: right;\">          0.653267</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.211409 </td><td style=\"text-align: right;\">      0.0907325</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  0.67475  </td><td style=\"text-align: right;\">     4.60326</td><td style=\"text-align: right;\">   0.967533</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3358.15</td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "<tr><td>objective_fee57690</td><td>TERMINATED</td><td>143.248.55.56:1112320</td><td style=\"text-align: right;\">           0.506905</td><td style=\"text-align: right;\">          0.57985 </td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.314181 </td><td style=\"text-align: right;\">      0.189641 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  1.57047  </td><td style=\"text-align: right;\">     0.56062</td><td style=\"text-align: right;\">   0.652699</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2609.23</td><td style=\"text-align: right;\">-0.518465</td><td style=\"text-align: right;\">0.518465</td></tr>\n",
       "<tr><td>objective_1727b044</td><td>TERMINATED</td><td>143.248.55.56:1112403</td><td style=\"text-align: right;\">           0.834803</td><td style=\"text-align: right;\">          0.820383</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.0128884</td><td style=\"text-align: right;\">      0.115461 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  0.0840583</td><td style=\"text-align: right;\">     2.33746</td><td style=\"text-align: right;\">   0.908787</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2822.32</td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "<tr><td>objective_26cffb68</td><td>TERMINATED</td><td>143.248.55.56:1112432</td><td style=\"text-align: right;\">           0.873858</td><td style=\"text-align: right;\">          0.885596</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.306706 </td><td style=\"text-align: right;\">      0.023659 </td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  0.641455 </td><td style=\"text-align: right;\">     4.45629</td><td style=\"text-align: right;\">   0.83049 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2453.85</td><td style=\"text-align: right;\">-0.518465</td><td style=\"text-align: right;\">0.518465</td></tr>\n",
       "<tr><td>objective_76f85f1c</td><td>TERMINATED</td><td>143.248.55.56:1112460</td><td style=\"text-align: right;\">           0.737163</td><td style=\"text-align: right;\">          0.618808</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.294065 </td><td style=\"text-align: right;\">      0.135944 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  0.170483 </td><td style=\"text-align: right;\">     1.75505</td><td style=\"text-align: right;\">   0.755823</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3265.83</td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "<tr><td>objective_00acf3b5</td><td>TERMINATED</td><td>143.248.55.56:1112488</td><td style=\"text-align: right;\">           0.523562</td><td style=\"text-align: right;\">          0.75547 </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.0445709</td><td style=\"text-align: right;\">      0.027051 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  1.59951  </td><td style=\"text-align: right;\">     1.9341 </td><td style=\"text-align: right;\">   0.842369</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3950.06</td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "<tr><td>objective_3c9c4f6b</td><td>TERMINATED</td><td>143.248.55.56:1112545</td><td style=\"text-align: right;\">           0.562178</td><td style=\"text-align: right;\">          0.52765 </td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.350528 </td><td style=\"text-align: right;\">      0.078416 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">  0.626999 </td><td style=\"text-align: right;\">     4.8294 </td><td style=\"text-align: right;\">   0.998031</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4921.46</td><td style=\"text-align: right;\">-0.518465</td><td style=\"text-align: right;\">0.518465</td></tr>\n",
       "<tr><td>objective_eca74e8c</td><td>TERMINATED</td><td>143.248.55.56:1112574</td><td style=\"text-align: right;\">           0.666246</td><td style=\"text-align: right;\">          0.964011</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.332918 </td><td style=\"text-align: right;\">      0.0158666</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  1.1303   </td><td style=\"text-align: right;\">     1.41074</td><td style=\"text-align: right;\">   0.949792</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2899.6 </td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "<tr><td>objective_0c7b0e86</td><td>TERMINATED</td><td>143.248.55.56:1112712</td><td style=\"text-align: right;\">           0.844859</td><td style=\"text-align: right;\">          0.611921</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.181036 </td><td style=\"text-align: right;\">      0.0203771</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  0.290485 </td><td style=\"text-align: right;\">     4.60863</td><td style=\"text-align: right;\">   0.938881</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2617.89</td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "<tr><td>objective_d0ee696f</td><td>TERMINATED</td><td>143.248.55.56:1112741</td><td style=\"text-align: right;\">           0.512621</td><td style=\"text-align: right;\">          0.998237</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.180717 </td><td style=\"text-align: right;\">      0.0272976</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">  0.176633 </td><td style=\"text-align: right;\">     4.17704</td><td style=\"text-align: right;\">   0.677175</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2707.17</td><td style=\"text-align: right;\">-0.518914</td><td style=\"text-align: right;\">0.518914</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_20 with AUC: 0.2946127946127946\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_19 with AUC: 0.74\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_20 with AUC: 0.2946127946127946\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_22 with AUC: 0.4838709677419355\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_23 with AUC: 0.535744322960471\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_22 with AUC: 0.4838709677419355\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_23 with AUC: 0.535744322960471\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_19 with AUC: 0.74\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_36 with AUC: 0.5792592592592593\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_19 with AUC: 0.74\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112432)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\n",
      "\u001b[2m\u001b[36m(objective pid=1112320)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112258)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112403)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112574)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_19 with AUC: 0.74\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112460)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112289)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112488)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_17 with AUC: 0.5851393188854489\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_19 with AUC: 0.74\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_7 with AUC: 0.5609756097560976\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_2 with AUC: 0.4263392857142857\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_8 with AUC: 0.6799999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_4 with AUC: 0.35306122448979593\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_23 with AUC: 0.543313708999159\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_9 with AUC: 0.46482758620689657\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_20 with AUC: 0.28787878787878785\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_21 with AUC: 0.5286195286195287\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_10 with AUC: 0.39629629629629626\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_27 with AUC: 0.3974576271186441\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112545)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_22 with AUC: 0.49379652605459057\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_28 with AUC: 0.5653409090909091\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_11 with AUC: 0.17647058823529416\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_29 with AUC: 0.5271739130434783\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_30 with AUC: 0.46825396825396814\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_12 with AUC: 0.5048701298701298\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_13 with AUC: 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_31 with AUC: 0.6448202959830867\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_32 with AUC: 0.5340136054421769\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_15 with AUC: 0.5727272727272728\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_24 with AUC: 0.5263157894736842\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_33 with AUC: 0.43209876543209874\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\n",
      "\u001b[2m\u001b[36m(objective pid=1112963)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_16 with AUC: 0.43589743589743596\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_34 with AUC: 0.5520833333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_35 with AUC: 0.5608974358974359\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_25 with AUC: 0.48333333333333334\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_18 with AUC: 0.6749999999999999\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112876)\u001b[0m Training completed for Fold_14 with AUC: 0.48415300546448087\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_36 with AUC: 0.5896296296296296\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\n",
      "\u001b[2m\u001b[36m(objective pid=1112913)\u001b[0m Training completed for Fold_19 with AUC: 0.74\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112963)\u001b[0m Training completed for Fold_3 with AUC: 0.6064814814814815\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_37 with AUC: 0.5559999999999999\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_40 with AUC: 0.6020408163265306\n",
      "\u001b[2m\u001b[36m(objective pid=1112774)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\n",
      "\u001b[2m\u001b[36m(objective pid=1112712)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\n",
      "\u001b[2m\u001b[36m(objective pid=1112845)\u001b[0m Training completed for Fold_26 with AUC: 0.6608187134502924\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_38 with AUC: 0.47089947089947093\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_41 with AUC: 0.6344827586206897\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112997)\u001b[0m Training completed for Fold_0 with AUC: 0.5329153605015674\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_44 with AUC: 0.5674603174603176\n",
      "\u001b[2m\u001b[36m(objective pid=1112963)\u001b[0m Training completed for Fold_5 with AUC: 0.5515873015873015\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_42 with AUC: 0.6481481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112741)\u001b[0m Training completed for Fold_46 with AUC: 0.5981481481481481\n",
      "\u001b[2m\u001b[36m(objective pid=1112661)\u001b[0m Training completed for Fold_45 with AUC: 0.5892857142857143\n",
      "\u001b[2m\u001b[36m(objective pid=1112997)\u001b[0m Training completed for Fold_1 with AUC: 0.4153439153439154\n",
      "\u001b[2m\u001b[36m(objective pid=1112808)\u001b[0m Training completed for Fold_39 with AUC: 0.4821428571428571\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 19:11:39,035\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=1112516)\u001b[0m Training completed for Fold_43 with AUC: 0.4979166666666667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=1112963)\u001b[0m Training completed for Fold_6 with AUC: 0.4734042553191489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 19:11:49,083\tINFO tune.py:1148 -- Total run time: 5379.57 seconds (5369.43 seconds for the tuning loop).\n",
      "2024-04-23 19:11:49,084\tWARNING tune.py:1163 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-04-23 19:11:49,160\tWARNING experiment_analysis.py:916 -- Failed to read the results for 11 trials:\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_9042ea90_8_colsample_bylevel=0.7380,colsample_bytree=0.5183,early_stopping_rounds=50,gamma=0.3072,learning_rate=0.0773,m_2024-04-23_17-42-20\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_e0abbe7c_11_colsample_bylevel=0.9430,colsample_bytree=0.9743,early_stopping_rounds=30,gamma=0.4965,learning_rate=0.0857,_2024-04-23_17-42-27\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_c5af3588_14_colsample_bylevel=0.6406,colsample_bytree=0.8620,early_stopping_rounds=30,gamma=0.1081,learning_rate=0.0310,_2024-04-23_18-25-52\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_ac060505_15_colsample_bylevel=0.6498,colsample_bytree=0.5472,early_stopping_rounds=50,gamma=0.1805,learning_rate=0.0362,_2024-04-23_18-29-20\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_378d2c63_16_colsample_bylevel=0.5756,colsample_bytree=0.8698,early_stopping_rounds=50,gamma=0.3529,learning_rate=0.0220,_2024-04-23_18-30-50\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_89abdd6f_17_colsample_bylevel=0.9225,colsample_bytree=0.6093,early_stopping_rounds=10,gamma=0.4021,learning_rate=0.0571,_2024-04-23_18-36-46\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_78e8377b_18_colsample_bylevel=0.6108,colsample_bytree=0.9031,early_stopping_rounds=50,gamma=0.4329,learning_rate=0.0832,_2024-04-23_18-38-12\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_3152a840_19_colsample_bylevel=0.7957,colsample_bytree=0.9430,early_stopping_rounds=10,gamma=0.2324,learning_rate=0.0950,_2024-04-23_18-48-13\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_28f3e760_20_colsample_bylevel=0.7089,colsample_bytree=0.6994,early_stopping_rounds=50,gamma=0.2955,learning_rate=0.0791,_2024-04-23_19-04-29\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_0481a670_21_colsample_bylevel=0.9819,colsample_bytree=0.7089,early_stopping_rounds=30,gamma=0.1215,learning_rate=0.0114,_2024-04-23_19-09-25\n",
      "- /home/panyu/ray_results/objective_2024-04-23_17-42-09/objective_aa03b477_22_colsample_bylevel=0.7883,colsample_bytree=0.6840,early_stopping_rounds=30,gamma=0.2369,learning_rate=0.0518,_2024-04-23_19-11-01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ray import tune\n",
    "from ray.tune import with_parameters\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import traceback\n",
    "import ray\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    metrics: dict\n",
    "    duration: float\n",
    "\n",
    "def log(message: str):\n",
    "    print(message)  # Simple logging to stdout or enhance as needed\n",
    "\n",
    "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if normalize:\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "        \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            \n",
    "        if select:\n",
    "            # # Removing low variance features\n",
    "            # X_train = exclude_low_variance(X_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            # #Removing highly correlated features\n",
    "            # X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            if isinstance(select, SelectFromModel):\n",
    "                select = [select]\n",
    "                \n",
    "            for i, s in enumerate(select):\n",
    "                C = np.asarray(X_train.columns)\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "\n",
    "        if oversample:\n",
    "            if len(C_cat) > 0:\n",
    "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        estimator = clone(estimator).fit(X_train, y_train)\n",
    "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
    "\n",
    "        result = FoldResult(\n",
    "            name=fold_name,\n",
    "            metrics={'AUC': auc_score},\n",
    "            duration=time.time() - start_time\n",
    "        )\n",
    "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
    "        return None\n",
    "\n",
    "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
    "    if not ray.is_initialized():\n",
    "        ray.init()\n",
    "\n",
    "    futures = []\n",
    "    splitter = LeaveOneGroupOut()  # Or any other CV strategy\n",
    "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        C_cat = np.asarray(sorted(cats))\n",
    "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "        job = train_fold('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
    "        futures.append(job)\n",
    "\n",
    "    results = futures\n",
    "    return results\n",
    "\n",
    "def objective(params, X, y, groups):\n",
    "    SELECT_LASSO = SelectFromModel(\n",
    "            estimator=LogisticRegression(\n",
    "            penalty='l1' \n",
    "            ,solver='liblinear'\n",
    "            , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "        ),\n",
    "        threshold = 0.005\n",
    "    )\n",
    "    # Example usage\n",
    "    estimator = EvXGBClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        eval_metric='logloss', \n",
    "        eval_size=0.2,\n",
    "        objective='binary:logistic', \n",
    "        verbosity=0,\n",
    "        **params\n",
    "    )  \n",
    "\n",
    "    results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
    "    auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
    "\n",
    "    mean_auc = np.mean(auc_values)\n",
    "    return {'loss': -mean_auc, 'auc': mean_auc, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 11)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 250, 500]),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.5, 5.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
    "    'num_parallel_tree': hp.choice('num_parallel_tree', [1, 10, 20]),\n",
    "    'early_stopping_rounds': hp.choice('early_stopping_rounds', [10, 30, 50]),\n",
    "}\n",
    "\n",
    "# Setup HyperOpt search with Ray Tune\n",
    "algo = HyperOptSearch(space, metric=\"auc\", mode=\"max\")\n",
    "\n",
    "# Define the scheduler for early stopping\n",
    "scheduler = HyperBandScheduler(time_attr=\"training_iteration\", metric=\"auc\", mode=\"max\")\n",
    "with on_ray():\n",
    "    # Assuming X, y, and groups are predefined datasets\n",
    "    analysis = tune.run(\n",
    "        with_parameters(objective, X=X, y=y, groups=groups),\n",
    "        num_samples=100,\n",
    "        search_alg=algo,\n",
    "        resources_per_trial={\"cpu\": 1},\n",
    "        verbose=1,\n",
    "        scheduler=scheduler\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'colsample_bylevel': 0.591680745868391, 'colsample_bytree': 0.6743168724873578, 'early_stopping_rounds': 30, 'gamma': 0.16718407941313662, 'learning_rate': 0.03451088913601569, 'max_depth': 9, 'min_child_weight': 4.0, 'n_estimators': 500, 'num_parallel_tree': 10, 'reg_alpha': 0.9674653063681278, 'reg_lambda': 3.3752464686948005, 'subsample': 0.747979943335319}\n",
      "Best trial AUC: -0.5189142967578859\n"
     ]
    }
   ],
   "source": [
    "# Explicitly specify the metric and mode when fetching the best trial\n",
    "best_trial = analysis.get_best_trial(metric=\"loss\", mode=\"min\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial AUC: {}\".format(best_trial.last_result[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-24 13:33:12</td></tr>\n",
       "<tr><td>Running for: </td><td>00:22:29.53        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.2/62.6 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using HyperBand: num_stopped=0 total_brackets=4<br>Round #0:<br>  Bracket(Max Size (n)=5, Milestone (r)=81, completed=0.0%): {RUNNING: 5} <br>  Bracket(Max Size (n)=8, Milestone (r)=27, completed=0.0%): {RUNNING: 8} <br>  Bracket(Max Size (n)=15, Milestone (r)=9, completed=0.0%): {RUNNING: 15} <br>  Bracket(Max Size (n)=34, Milestone (r)=3, completed=0.0%): {PENDING: 1} <br>Logical resource usage: 28.0/28 CPUs, 0/2 GPUs (0.0/2.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  colsample_bylevel</th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">   early_stopping_round\n",
       "s</th><th style=\"text-align: right;\">    gamma</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  num_parallel_tree</th><th style=\"text-align: right;\">  random_state</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  subsample</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_44c3a743</td><td>RUNNING </td><td>143.248.57.67:697229</td><td style=\"text-align: right;\">           0.635019</td><td style=\"text-align: right;\">          0.982946</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.399829 </td><td style=\"text-align: right;\">      0.0321711</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.504084</td><td style=\"text-align: right;\">    0.798338</td><td style=\"text-align: right;\">   0.940269</td></tr>\n",
       "<tr><td>objective_afb11107</td><td>RUNNING </td><td>143.248.57.67:697261</td><td style=\"text-align: right;\">           0.769844</td><td style=\"text-align: right;\">          0.730624</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.396174 </td><td style=\"text-align: right;\">      0.122778 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.731552</td><td style=\"text-align: right;\">    3.34528 </td><td style=\"text-align: right;\">   0.972663</td></tr>\n",
       "<tr><td>objective_a0f279d4</td><td>RUNNING </td><td>143.248.57.67:697291</td><td style=\"text-align: right;\">           0.553627</td><td style=\"text-align: right;\">          0.644205</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.0553617</td><td style=\"text-align: right;\">      0.0340857</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.153877</td><td style=\"text-align: right;\">    3.78661 </td><td style=\"text-align: right;\">   0.63673 </td></tr>\n",
       "<tr><td>objective_f8acc38a</td><td>RUNNING </td><td>143.248.57.67:697330</td><td style=\"text-align: right;\">           0.675823</td><td style=\"text-align: right;\">          0.712073</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.181368 </td><td style=\"text-align: right;\">      0.143883 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.99831 </td><td style=\"text-align: right;\">    4.70897 </td><td style=\"text-align: right;\">   0.815795</td></tr>\n",
       "<tr><td>objective_f21fb950</td><td>RUNNING </td><td>143.248.57.67:697361</td><td style=\"text-align: right;\">           0.976541</td><td style=\"text-align: right;\">          0.893745</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.127038 </td><td style=\"text-align: right;\">      0.130837 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.03689 </td><td style=\"text-align: right;\">    2.17866 </td><td style=\"text-align: right;\">   0.803362</td></tr>\n",
       "<tr><td>objective_0a932712</td><td>RUNNING </td><td>143.248.57.67:697392</td><td style=\"text-align: right;\">           0.597565</td><td style=\"text-align: right;\">          0.718765</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.0244048</td><td style=\"text-align: right;\">      0.0270392</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.250676</td><td style=\"text-align: right;\">    1.38163 </td><td style=\"text-align: right;\">   0.825668</td></tr>\n",
       "<tr><td>objective_b7c6d19c</td><td>RUNNING </td><td>143.248.57.67:697425</td><td style=\"text-align: right;\">           0.554073</td><td style=\"text-align: right;\">          0.702175</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.0567406</td><td style=\"text-align: right;\">      0.0135583</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.576851</td><td style=\"text-align: right;\">    0.579164</td><td style=\"text-align: right;\">   0.890033</td></tr>\n",
       "<tr><td>objective_f89779a8</td><td>RUNNING </td><td>143.248.57.67:697456</td><td style=\"text-align: right;\">           0.838325</td><td style=\"text-align: right;\">          0.893837</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.412447 </td><td style=\"text-align: right;\">      0.0111574</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.650284</td><td style=\"text-align: right;\">    1.58522 </td><td style=\"text-align: right;\">   0.952054</td></tr>\n",
       "<tr><td>objective_377b03b4</td><td>RUNNING </td><td>143.248.57.67:697486</td><td style=\"text-align: right;\">           0.859235</td><td style=\"text-align: right;\">          0.728791</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.427961 </td><td style=\"text-align: right;\">      0.0253313</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.06547 </td><td style=\"text-align: right;\">    2.82633 </td><td style=\"text-align: right;\">   0.69812 </td></tr>\n",
       "<tr><td>objective_8080c696</td><td>RUNNING </td><td>143.248.57.67:697518</td><td style=\"text-align: right;\">           0.964795</td><td style=\"text-align: right;\">          0.915999</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.218422 </td><td style=\"text-align: right;\">      0.0635374</td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.341947</td><td style=\"text-align: right;\">    2.56801 </td><td style=\"text-align: right;\">   0.890033</td></tr>\n",
       "<tr><td>objective_f9fa7eff</td><td>RUNNING </td><td>143.248.57.67:697549</td><td style=\"text-align: right;\">           0.991699</td><td style=\"text-align: right;\">          0.587877</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.494135 </td><td style=\"text-align: right;\">      0.102814 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.17941 </td><td style=\"text-align: right;\">    2.29004 </td><td style=\"text-align: right;\">   0.604021</td></tr>\n",
       "<tr><td>objective_d6d5a919</td><td>RUNNING </td><td>143.248.57.67:697580</td><td style=\"text-align: right;\">           0.867638</td><td style=\"text-align: right;\">          0.732795</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.173436 </td><td style=\"text-align: right;\">      0.195927 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.4178  </td><td style=\"text-align: right;\">    3.05576 </td><td style=\"text-align: right;\">   0.983998</td></tr>\n",
       "<tr><td>objective_3fd5fac0</td><td>RUNNING </td><td>143.248.57.67:697611</td><td style=\"text-align: right;\">           0.564129</td><td style=\"text-align: right;\">          0.720486</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.236201 </td><td style=\"text-align: right;\">      0.0166831</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.98351 </td><td style=\"text-align: right;\">    1.70034 </td><td style=\"text-align: right;\">   0.754533</td></tr>\n",
       "<tr><td>objective_894cea25</td><td>RUNNING </td><td>143.248.57.67:697642</td><td style=\"text-align: right;\">           0.539607</td><td style=\"text-align: right;\">          0.825602</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.0860898</td><td style=\"text-align: right;\">      0.163686 </td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.30738 </td><td style=\"text-align: right;\">    4.61029 </td><td style=\"text-align: right;\">   0.997143</td></tr>\n",
       "<tr><td>objective_abe7ebf6</td><td>RUNNING </td><td>143.248.57.67:697673</td><td style=\"text-align: right;\">           0.942002</td><td style=\"text-align: right;\">          0.993925</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.415233 </td><td style=\"text-align: right;\">      0.195025 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.91698 </td><td style=\"text-align: right;\">    3.13529 </td><td style=\"text-align: right;\">   0.884522</td></tr>\n",
       "<tr><td>objective_346e7c63</td><td>RUNNING </td><td>143.248.57.67:697704</td><td style=\"text-align: right;\">           0.530609</td><td style=\"text-align: right;\">          0.989363</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.198991 </td><td style=\"text-align: right;\">      0.0309865</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.116362</td><td style=\"text-align: right;\">    3.06023 </td><td style=\"text-align: right;\">   0.776368</td></tr>\n",
       "<tr><td>objective_3ceeb9ec</td><td>RUNNING </td><td>143.248.57.67:697735</td><td style=\"text-align: right;\">           0.956001</td><td style=\"text-align: right;\">          0.519873</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.238328 </td><td style=\"text-align: right;\">      0.0138698</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.09946 </td><td style=\"text-align: right;\">    0.508324</td><td style=\"text-align: right;\">   0.892099</td></tr>\n",
       "<tr><td>objective_4708d801</td><td>RUNNING </td><td>143.248.57.67:697766</td><td style=\"text-align: right;\">           0.632757</td><td style=\"text-align: right;\">          0.700919</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.409232 </td><td style=\"text-align: right;\">      0.172585 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.123733</td><td style=\"text-align: right;\">    1.05669 </td><td style=\"text-align: right;\">   0.607699</td></tr>\n",
       "<tr><td>objective_d01cbfd0</td><td>RUNNING </td><td>143.248.57.67:697797</td><td style=\"text-align: right;\">           0.577879</td><td style=\"text-align: right;\">          0.627172</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.0783369</td><td style=\"text-align: right;\">      0.0101577</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.6145  </td><td style=\"text-align: right;\">    3.78345 </td><td style=\"text-align: right;\">   0.644509</td></tr>\n",
       "<tr><td>objective_1d58b6f3</td><td>RUNNING </td><td>143.248.57.67:697828</td><td style=\"text-align: right;\">           0.788548</td><td style=\"text-align: right;\">          0.816609</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.297599 </td><td style=\"text-align: right;\">      0.194024 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.45142 </td><td style=\"text-align: right;\">    1.32803 </td><td style=\"text-align: right;\">   0.700049</td></tr>\n",
       "<tr><td>objective_cc0f9ac7</td><td>RUNNING </td><td>143.248.57.67:697859</td><td style=\"text-align: right;\">           0.657681</td><td style=\"text-align: right;\">          0.79333 </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.319903 </td><td style=\"text-align: right;\">      0.0644468</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.432518</td><td style=\"text-align: right;\">    0.914464</td><td style=\"text-align: right;\">   0.929946</td></tr>\n",
       "<tr><td>objective_0210b08c</td><td>RUNNING </td><td>143.248.57.67:697890</td><td style=\"text-align: right;\">           0.712375</td><td style=\"text-align: right;\">          0.52454 </td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.489607 </td><td style=\"text-align: right;\">      0.0192013</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.60343 </td><td style=\"text-align: right;\">    4.16029 </td><td style=\"text-align: right;\">   0.679897</td></tr>\n",
       "<tr><td>objective_eee002de</td><td>RUNNING </td><td>143.248.57.67:697921</td><td style=\"text-align: right;\">           0.77751 </td><td style=\"text-align: right;\">          0.946025</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.307849 </td><td style=\"text-align: right;\">      0.0510052</td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.774747</td><td style=\"text-align: right;\">    0.960923</td><td style=\"text-align: right;\">   0.725839</td></tr>\n",
       "<tr><td>objective_c697f156</td><td>RUNNING </td><td>143.248.57.67:697952</td><td style=\"text-align: right;\">           0.650094</td><td style=\"text-align: right;\">          0.809824</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.352345 </td><td style=\"text-align: right;\">      0.0782589</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.436003</td><td style=\"text-align: right;\">    0.823285</td><td style=\"text-align: right;\">   0.953696</td></tr>\n",
       "<tr><td>objective_ecf05d62</td><td>RUNNING </td><td>143.248.57.67:697984</td><td style=\"text-align: right;\">           0.710655</td><td style=\"text-align: right;\">          0.539229</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.499829 </td><td style=\"text-align: right;\">      0.0207929</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.7981  </td><td style=\"text-align: right;\">    4.27113 </td><td style=\"text-align: right;\">   0.86082 </td></tr>\n",
       "<tr><td>objective_87f8c5d4</td><td>RUNNING </td><td>143.248.57.67:698014</td><td style=\"text-align: right;\">           0.798524</td><td style=\"text-align: right;\">          0.946004</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.276635 </td><td style=\"text-align: right;\">      0.0421228</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.835431</td><td style=\"text-align: right;\">    1.72211 </td><td style=\"text-align: right;\">   0.746483</td></tr>\n",
       "<tr><td>objective_58b8bfa9</td><td>RUNNING </td><td>143.248.57.67:698046</td><td style=\"text-align: right;\">           0.61702 </td><td style=\"text-align: right;\">          0.853159</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.348542 </td><td style=\"text-align: right;\">      0.087494 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.468737</td><td style=\"text-align: right;\">    0.538648</td><td style=\"text-align: right;\">   0.930572</td></tr>\n",
       "<tr><td>objective_ca1c0918</td><td>RUNNING </td><td>143.248.57.67:698078</td><td style=\"text-align: right;\">           0.733824</td><td style=\"text-align: right;\">          0.584319</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.454511 </td><td style=\"text-align: right;\">      0.0205361</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   1.80902 </td><td style=\"text-align: right;\">    4.32961 </td><td style=\"text-align: right;\">   0.846157</td></tr>\n",
       "<tr><td>objective_afdb2a1e</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">           0.814862</td><td style=\"text-align: right;\">          0.956716</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.377634 </td><td style=\"text-align: right;\">      0.0376297</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">            42</td><td style=\"text-align: right;\">   0.851561</td><td style=\"text-align: right;\">    1.96702 </td><td style=\"text-align: right;\">   0.756227</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 13:33:12,519\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-04-24 13:33:22,555\tINFO tune.py:1148 -- Total run time: 1359.57 seconds (1349.52 seconds for the tuning loop).\n",
      "2024-04-24 13:33:22,556\tWARNING tune.py:1163 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-04-24 13:33:22,560\tWARNING experiment_analysis.py:916 -- Failed to read the results for 29 trials:\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_44c3a743_1_colsample_bylevel=0.6350,colsample_bytree=0.9829,early_stopping_rounds=30,gamma=0.3998,learning_rate=0.0322,m_2024-04-24_13-10-43\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_afb11107_2_colsample_bylevel=0.7698,colsample_bytree=0.7306,early_stopping_rounds=10,gamma=0.3962,learning_rate=0.1228,m_2024-04-24_13-10-44\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_a0f279d4_3_colsample_bylevel=0.5536,colsample_bytree=0.6442,early_stopping_rounds=10,gamma=0.0554,learning_rate=0.0341,m_2024-04-24_13-10-45\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_f8acc38a_4_colsample_bylevel=0.6758,colsample_bytree=0.7121,early_stopping_rounds=50,gamma=0.1814,learning_rate=0.1439,m_2024-04-24_13-10-46\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_f21fb950_5_colsample_bylevel=0.9765,colsample_bytree=0.8937,early_stopping_rounds=30,gamma=0.1270,learning_rate=0.1308,m_2024-04-24_13-10-47\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_0a932712_6_colsample_bylevel=0.5976,colsample_bytree=0.7188,early_stopping_rounds=10,gamma=0.0244,learning_rate=0.0270,m_2024-04-24_13-10-47\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_b7c6d19c_7_colsample_bylevel=0.5541,colsample_bytree=0.7022,early_stopping_rounds=50,gamma=0.0567,learning_rate=0.0136,m_2024-04-24_13-10-48\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_f89779a8_8_colsample_bylevel=0.8383,colsample_bytree=0.8938,early_stopping_rounds=10,gamma=0.4124,learning_rate=0.0112,m_2024-04-24_13-10-49\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_377b03b4_9_colsample_bylevel=0.8592,colsample_bytree=0.7288,early_stopping_rounds=50,gamma=0.4280,learning_rate=0.0253,m_2024-04-24_13-10-50\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_8080c696_10_colsample_bylevel=0.9648,colsample_bytree=0.9160,early_stopping_rounds=50,gamma=0.2184,learning_rate=0.0635,_2024-04-24_13-10-52\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_f9fa7eff_11_colsample_bylevel=0.9917,colsample_bytree=0.5879,early_stopping_rounds=30,gamma=0.4941,learning_rate=0.1028,_2024-04-24_13-10-53\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_d6d5a919_12_colsample_bylevel=0.8676,colsample_bytree=0.7328,early_stopping_rounds=50,gamma=0.1734,learning_rate=0.1959,_2024-04-24_13-10-55\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_3fd5fac0_13_colsample_bylevel=0.5641,colsample_bytree=0.7205,early_stopping_rounds=10,gamma=0.2362,learning_rate=0.0167,_2024-04-24_13-10-56\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_894cea25_14_colsample_bylevel=0.5396,colsample_bytree=0.8256,early_stopping_rounds=30,gamma=0.0861,learning_rate=0.1637,_2024-04-24_13-10-58\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_abe7ebf6_15_colsample_bylevel=0.9420,colsample_bytree=0.9939,early_stopping_rounds=30,gamma=0.4152,learning_rate=0.1950,_2024-04-24_13-10-59\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_346e7c63_16_colsample_bylevel=0.5306,colsample_bytree=0.9894,early_stopping_rounds=10,gamma=0.1990,learning_rate=0.0310,_2024-04-24_13-11-01\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_3ceeb9ec_17_colsample_bylevel=0.9560,colsample_bytree=0.5199,early_stopping_rounds=30,gamma=0.2383,learning_rate=0.0139,_2024-04-24_13-11-02\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_4708d801_18_colsample_bylevel=0.6328,colsample_bytree=0.7009,early_stopping_rounds=50,gamma=0.4092,learning_rate=0.1726,_2024-04-24_13-11-04\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_d01cbfd0_19_colsample_bylevel=0.5779,colsample_bytree=0.6272,early_stopping_rounds=30,gamma=0.0783,learning_rate=0.0102,_2024-04-24_13-11-05\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_1d58b6f3_20_colsample_bylevel=0.7885,colsample_bytree=0.8166,early_stopping_rounds=10,gamma=0.2976,learning_rate=0.1940,_2024-04-24_13-11-07\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_cc0f9ac7_21_colsample_bylevel=0.6577,colsample_bytree=0.7933,early_stopping_rounds=50,gamma=0.3199,learning_rate=0.0644,_2024-04-24_13-11-08\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_0210b08c_22_colsample_bylevel=0.7124,colsample_bytree=0.5245,early_stopping_rounds=30,gamma=0.4896,learning_rate=0.0192,_2024-04-24_13-11-10\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_eee002de_23_colsample_bylevel=0.7775,colsample_bytree=0.9460,early_stopping_rounds=10,gamma=0.3078,learning_rate=0.0510,_2024-04-24_13-11-12\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_c697f156_24_colsample_bylevel=0.6501,colsample_bytree=0.8098,early_stopping_rounds=50,gamma=0.3523,learning_rate=0.0783,_2024-04-24_13-11-13\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_ecf05d62_25_colsample_bylevel=0.7107,colsample_bytree=0.5392,early_stopping_rounds=30,gamma=0.4998,learning_rate=0.0208,_2024-04-24_13-11-15\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_87f8c5d4_26_colsample_bylevel=0.7985,colsample_bytree=0.9460,early_stopping_rounds=10,gamma=0.2766,learning_rate=0.0421,_2024-04-24_13-11-16\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_58b8bfa9_27_colsample_bylevel=0.6170,colsample_bytree=0.8532,early_stopping_rounds=50,gamma=0.3485,learning_rate=0.0875,_2024-04-24_13-11-17\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_ca1c0918_28_colsample_bylevel=0.7338,colsample_bytree=0.5843,early_stopping_rounds=30,gamma=0.4545,learning_rate=0.0205,_2024-04-24_13-11-19\n",
      "- /home/iclab/ray_results/objective_2024-04-24_13-10-42/objective_afdb2a1e_29_colsample_bylevel=0.8149,colsample_bytree=0.9567,early_stopping_rounds=10,gamma=0.3776,learning_rate=0.0376,_2024-04-24_13-11-20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from ray import tune\n",
    "from ray.tune import with_parameters\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "#     estimator=LinearSVC(\n",
    "#         penalty='l1',\n",
    "#         loss='squared_hinge',\n",
    "#         dual=False,\n",
    "#         tol=1e-3,\n",
    "#         C=1e-2,\n",
    "#         max_iter=5000,\n",
    "#         random_state=RANDOM_STATE\n",
    "#     ),\n",
    "#     threshold=1e-5\n",
    "    \n",
    "        estimator=LogisticRegression(\n",
    "        penalty='l1' \n",
    "        ,solver='liblinear'\n",
    "        , C=1, random_state=42, max_iter=4000\n",
    "    ),\n",
    "    threshold = 0.005\n",
    ")\n",
    "\n",
    "\n",
    "def objective(params, X, y, groups):\n",
    "    scaler = StandardScaler()\n",
    "    logo = LeaveOneGroupOut()\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, test_idx in logo.split(X, y, groups):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        ##########################\n",
    "        normalize = True\n",
    "        select = [SELECT_SVC]\n",
    "        oversample = True\n",
    "        random_state = 42\n",
    "\n",
    "\n",
    "        C_cat = np.asarray(sorted(cats))\n",
    "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "\n",
    "        if normalize:\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "        \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            \n",
    "        if select:\n",
    "            # # Removing low variance features\n",
    "            # X_train = exclude_low_variance(X_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            # #Removing highly correlated features\n",
    "            # X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            if isinstance(select, SelectFromModel):\n",
    "                select = [select]\n",
    "                \n",
    "            for i, s in enumerate(select):\n",
    "                C = np.asarray(X_train.columns)\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "\n",
    "        if oversample:\n",
    "            if len(C_cat) > 0:\n",
    "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        ##########################\n",
    "\n",
    "\n",
    "        # # Split training data for early stopping\n",
    "        # X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "        #     X_train, y_train, test_size=0.2, random_state=int(params['random_state'])\n",
    "        # )\n",
    "        \n",
    "        # # Classifier\n",
    "        # clf = xgb.XGBClassifier(\n",
    "        #     objective='binary:logistic',\n",
    "        #     eval_metric='auc',\n",
    "        #     verbosity=0,\n",
    "        #     tree_method='hist',\n",
    "        #     **params\n",
    "        # )\n",
    "        # clf.fit(\n",
    "        #     X_train_sub, y_train_sub,\n",
    "        #     eval_set=[(X_val, y_val)],\n",
    "        #     verbose=False\n",
    "        # )\n",
    "\n",
    "        # best_iteration = clf.best_iteration\n",
    "        # y_pred = clf.predict_proba(X_test, iteration_range=(0, best_iteration + 1))[:, 1]\n",
    "        # auc_score = roc_auc_score(y_test, y_pred)\n",
    "        estimator = EvXGBClassifier(\n",
    "        eval_metric='logloss', \n",
    "        eval_size=0.2,\n",
    "        method='hist-gpu',\n",
    "        objective='binary:logistic', \n",
    "        verbosity=0,\n",
    "        **params\n",
    "        )  \n",
    "        estimator = clone(estimator).fit(X_train, y_train)\n",
    "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
    "        \n",
    "        auc_scores.append(auc_score)\n",
    "\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    return {'loss': -mean_auc, 'auc': mean_auc, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 11)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 250, 500]),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.5, 5.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
    "    'num_parallel_tree': hp.choice('num_parallel_tree', [1, 10, 20]),\n",
    "    'early_stopping_rounds': hp.choice('early_stopping_rounds', [10, 30, 50]),\n",
    "    'random_state': hp.choice('random_state', [42])\n",
    "}\n",
    "\n",
    "# Setup HyperOpt search with Ray Tune\n",
    "algo = HyperOptSearch(space, metric=\"auc\", mode=\"max\")\n",
    "\n",
    "# Define the scheduler for early stopping\n",
    "scheduler = HyperBandScheduler(time_attr=\"training_iteration\", metric=\"auc\", mode=\"max\")\n",
    "with on_ray():\n",
    "    # Assuming X, y, and groups are predefined datasets\n",
    "    analysis = tune.run(\n",
    "        with_parameters(objective, X=X, y=y, groups=groups),\n",
    "        num_samples=100,\n",
    "        search_alg=algo,\n",
    "        # resources_per_trial={\"cpu\": 1},\n",
    "        verbose=1,\n",
    "        scheduler=scheduler\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'colsample_bylevel': 0.681073126404533, 'colsample_bytree': 0.6443005293308781, 'early_stopping_rounds': 30, 'gamma': 0.28891067270936654, 'learning_rate': 0.08848040162909752, 'max_depth': 8, 'min_child_weight': 2.0, 'n_estimators': 100, 'num_parallel_tree': 1, 'random_state': 42, 'reg_alpha': 1.0787212485467532, 'reg_lambda': 1.856673415371724, 'subsample': 0.67266853756236}\n",
      "Best trial AUC: -0.5189142967578859\n"
     ]
    }
   ],
   "source": [
    "# Explicitly specify the metric and mode when fetching the best trial\n",
    "best_trial = analysis.get_best_trial(metric=\"loss\", mode=\"min\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial AUC: {}\".format(best_trial.last_result[\"loss\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
