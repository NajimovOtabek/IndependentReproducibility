{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_low_variance(agg_feature, threshold=.0000001):\n",
    "    agg_feature_non_zero_var = agg_feature.loc[:,agg_feature.var()>threshold]\n",
    "    num_removed = agg_feature.shape[1]-agg_feature_non_zero_var.shape[1]\n",
    "    print(f'{num_removed}/{agg_feature.shape[1]} features with variance < {threshold} removed')\n",
    "    return agg_feature_non_zero_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def remove_pairwise_corr(agg_feature_percent_missing, PAIRWISE_CORR_THRESHOLD=0.8, outcome_variable=None):\n",
    "    if outcome_variable is not None:\n",
    "        outcome_variable = pd.Series(outcome_variable, index=agg_feature_percent_missing.index, name=\"outcome\")\n",
    "        corr_with_outcome = pd.merge(outcome_variable, agg_feature_percent_missing, left_index=True, right_index=True).corr()[outcome_variable.name].abs().sort_values(ascending=False)\n",
    "        importance_order = corr_with_outcome.index[1:].tolist()\n",
    "        agg_feature_percent_missing = agg_feature_percent_missing[importance_order]\n",
    "\n",
    "    Matrix = agg_feature_percent_missing.corr().abs()\n",
    "    \n",
    "    upper_triangle = Matrix.where(np.triu(np.ones(Matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    correlated_features = set()\n",
    "    for feature in upper_triangle.columns:\n",
    "        highly_correlated = upper_triangle[feature][upper_triangle[feature] > PAIRWISE_CORR_THRESHOLD].index\n",
    "        correlated_features.update(highly_correlated)\n",
    "\n",
    "    kept_features = list(set(agg_feature_percent_missing.columns) - correlated_features)\n",
    "    print(f\"Pairwise Corr: kept only {len(kept_features)}/{len(agg_feature_percent_missing.columns)} features\")\n",
    "    return agg_feature_percent_missing[kept_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "class CustomCV:\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y, groups):\n",
    "        logo = LeaveOneGroupOut()\n",
    "\n",
    "        for train_users, test_users in logo.split(X, y, groups):\n",
    "            X_train_users, X_test_user = X.loc[train_users], X.loc[test_users]\n",
    "            y_train_users, y_test_user = y[train_users], y[test_users]\n",
    "            group_train_users, group_test_user = groups[train_users], groups[test_users]\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=self.n_splits) \n",
    "\n",
    "            # only take the first split\n",
    "            train_index, test_index = next(tscv.split(X_test_user))\n",
    "            \n",
    "            X_train, X_test = pd.concat([X_train_users, X_test_user.iloc[train_index]]), X_test_user.iloc[test_index]\n",
    "            y_train, y_test = np.concatenate([y_train_users, y_test_user[train_index]]), y_test_user[test_index]\n",
    "\n",
    "            yield (X_train.index, X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM\n",
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "    datetimes_train: np.ndarray = None\n",
    "    datetimes_test: np.ndarray = None\n",
    "\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    elif alg == 'TimeSeriesSplit':\n",
    "        splitter = TimeSeriesSplit(n_splits=n_splits)\n",
    "    elif alg == 'custom_cv':\n",
    "        splitter = CustomCV(n_splits=n_splits)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", \"TimeSeriesSplit\", \"custom_cv\" or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    datetimes_train: np.ndarray,  # Add datetimes_train parameter\n",
    "    datetimes_test: np.ndarray,  # Add datetimes_test parameter\n",
    "\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "\n",
    "\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "        #Instead of using fixed threshold, we tried to use mean value as threshold for binarization\n",
    "# #     y_train_mean = np.mean(np.concatenate((y_train,y_test)))\n",
    "#     y_train_mean = np.mean(y_train)\n",
    "#     y_train = np.where(y_train > y_train_mean, 1, 0)\n",
    "#     y_test= np.where(y_test > y_train_mean, 1, 0)\n",
    "# #     X_train['ESM#LastLabel'] = np.where(X_train['ESM#LastLabel'] > y_train_mean, 1, 0)\n",
    "# #     X_test['ESM#LastLabel'] = np.where(X_test['ESM#LastLabel'] > y_train_mean, 1, 0)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "        # Removing low variance features\n",
    "#         X_train = exclude_low_variance(X_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "        # Removing highly correlated features\n",
    "#         X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "#             # Create oversampled datetimes_train\n",
    "#             datetimes_train_oversampled = np.repeat(datetimes_train, sampler.sample_indices_.shape[0], axis=0)\n",
    "\n",
    "    # You can access the underlying model class like this:\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    datetimes:  np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "    \n",
    "    \n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "        datetimes_train, datetimes_test = datetimes[I_train], datetimes[I_test]  # Add datetimes_train and datetimes_test\n",
    "\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            \n",
    "            datetimes_train=datetimes_train,  # Pass datetimes_train\n",
    "            datetimes_test=datetimes_test,  # Pass datetimes_test\n",
    "\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'proc', 'LABELS_PROC.csv'), index_col=['pcode','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 15:16:26,460\tINFO worker.py:1452 -- Connecting to existing Ray cluster at address: 192.168.1.28:6379...\n",
      "2023-09-06 15:16:26,470\tINFO worker.py:1627 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=1139346)\u001b[0m [23-09-06 15:16:27] In progress: [xgb_os#P02] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=349620, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:27] Success: [xgb_os#P31] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=349620, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:27] In progress: [xgb_os#P31] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:41] Success: [xgb_os#P39] 1-th Feature selection (12.43s).\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 0; # Num. = 571)\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 423 (# Cat. = 0; # Num. = 423)\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:41] In progress: [xgb_os#P39] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=349813, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:29] In progress: [xgb_os#P49] Normalizing numeric features.\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349813, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:29] Success: [xgb_os#P49] Normalizing numeric features (0.05s).\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349813, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:29] In progress: [xgb_os#P49] 1-th Feature selection.\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:41] Success: [xgb_os#P39] Oversampling (0.17s).\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:41] In progress: [xgb_os#P39] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=349663, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:46] Success: [xgb_os#P32] 1-th Feature selection (18.09s).\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349663, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 0; # Num. = 571)\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349663, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 416 (# Cat. = 0; # Num. = 416)\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349663, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:46] In progress: [xgb_os#P32] Oversampling.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349663, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:46] Success: [xgb_os#P32] Oversampling (0.13s).\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349663, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:46] In progress: [xgb_os#P32] Training.\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139374)\u001b[0m [23-09-06 15:16:55] Success: [xgb_os#P08] Training (12.58s).\n",
      "\u001b[2m\u001b[36m(_train pid=349796, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:47] Success: [xgb_os#P47] 1-th Feature selection (17.89s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349796, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 0; # Num. = 571)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349796, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 430 (# Cat. = 0; # Num. = 430)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349796, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:47] In progress: [xgb_os#P47] Oversampling.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349796, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:55] Success: [xgb_os#P08] Training (12.58s).\n",
      "\u001b[2m\u001b[36m(_train pid=349796, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:55] Success: [xgb_os#P08] Training (12.58s).\n",
      "\u001b[2m\u001b[36m(_train pid=1139374)\u001b[0m [23-09-06 15:16:55] In progress: [xgb_os#P51] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=1139374)\u001b[0m [23-09-06 15:16:55] Success: [xgb_os#P51] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=1139374)\u001b[0m [23-09-06 15:16:55] In progress: [xgb_os#P51] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=349813, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:00] Success: [xgb_os#P49] Training (14.17s).\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349621, ip=192.168.1.27)\u001b[0m [23-09-06 15:16:59] In progress: [xgb_os#P78] Normalizing numeric features.\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349813, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:00] Success: [xgb_os#P79] Normalizing numeric features (0.03s).\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349813, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:00] In progress: [xgb_os#P79] 1-th Feature selection.\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:05] Success: [xgb_os#P52] 1-th Feature selection (7.40s).\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 0; # Num. = 571)\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m - # Sel. Feat.: 430 (# Cat. = 0; # Num. = 430)\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:05] In progress: [xgb_os#P52] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:05] Success: [xgb_os#P52] Oversampling (0.06s).\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:05] In progress: [xgb_os#P52] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=349796, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:02] Success: [xgb_os#P47] Training (14.60s).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349813, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:05] In progress: [xgb_os#P79] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1139416)\u001b[0m [23-09-06 15:17:10] Success: [xgb_os#P67] 1-th Feature selection (13.07s).\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139416)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 0; # Num. = 571)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139416)\u001b[0m - # Sel. Feat.: 427 (# Cat. = 0; # Num. = 427)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139416)\u001b[0m [23-09-06 15:17:10] In progress: [xgb_os#P67] Oversampling.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139353)\u001b[0m [23-09-06 15:17:09] Success: [xgb_os#P55] Oversampling (0.12s).\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139353)\u001b[0m [23-09-06 15:17:09] In progress: [xgb_os#P55] Training.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=349702, ip=192.168.1.27)\u001b[0m [23-09-06 15:17:13] Success: [xgb_os#P52] Training (8.51s).\n",
      "\u001b[2m\u001b[36m(_train pid=1139381)\u001b[0m [23-09-06 15:17:14] Success: [xgb_os#P75] 1-th Feature selection (14.44s).\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139381)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 0; # Num. = 571)\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139381)\u001b[0m - # Sel. Feat.: 424 (# Cat. = 0; # Num. = 424)\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139381)\u001b[0m [23-09-06 15:17:14] In progress: [xgb_os#P75] Oversampling.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139381)\u001b[0m [23-09-06 15:17:14] Success: [xgb_os#P75] Oversampling (0.10s).\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139381)\u001b[0m [23-09-06 15:17:14] In progress: [xgb_os#P75] Training.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139436)\u001b[0m [23-09-06 15:17:19] Success: [xgb_os#P66] Training (13.35s).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139338)\u001b[0m [23-09-06 15:17:24] Success: [xgb_os#P70] Training (12.66s).\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1139409)\u001b[0m [23-09-06 15:17:28] Success: [xgb_os#P60] Training (14.82s).\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "ESTIMATOR_LR = LogisticRegression(random_state = RANDOM_STATE, max_iter=500 )\n",
    "ESTIMATOR_KNN = KNeighborsClassifier()\n",
    "ESTIMATOR_SVM = SVC(probability=True)\n",
    "ESTIMATOR_GP = GaussianProcessClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_DT = DecisionTreeClassifier(random_state = RANDOM_STATE)\n",
    "ESTIMATOR_MLP = MLPClassifier(random_state=RANDOM_STATE, max_iter=2000)\n",
    "ESTIMATOR_ADAB = AdaBoostClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_GNB = GaussianNB()\n",
    "ESTIMATOR_QDA = QuadraticDiscriminantAnalysis()\n",
    "# ESTIMATOR_LSTM = LSTMWrapper()\n",
    "\n",
    "  \n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "#     estimator=LinearSVC(\n",
    "#         penalty='l1',\n",
    "#         loss='squared_hinge',\n",
    "#         dual=False,\n",
    "#         tol=1e-3,\n",
    "#         C=1e-2,\n",
    "#         max_iter=5000,\n",
    "#         random_state=RANDOM_STATE\n",
    "#     ),\n",
    "    \n",
    "        estimator=LogisticRegression(\n",
    "        penalty='l1' \n",
    "        ,solver='liblinear'\n",
    "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "    ),\n",
    "#     threshold=1e-5\n",
    "    threshold = 0.005\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "\n",
    "SETTINGS = [\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_DUMMY),\n",
    "#         oversample=False,\n",
    "#         select=None,\n",
    "#         name='dummy'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_RF),\n",
    "#         oversample=False,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='rf_ns'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_RF),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='rf_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_XGB),\n",
    "#         oversample=False,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='xgb_ns'\n",
    "#     ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_LR),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='lr_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_KNN),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='knn_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_SVM),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='svm_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_DT),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='dt_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_MLP),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='mlp_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_ADAB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='adab_os'\n",
    "#     ),\n",
    "#         dict(\n",
    "#         estimator=clone(ESTIMATOR_GP),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gp_os'\n",
    "#     ),\n",
    "#             dict(\n",
    "#         estimator=clone(ESTIMATOR_GNB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gnb_os'\n",
    "#     ),\n",
    "#             dict(\n",
    "#         estimator=clone(ESTIMATOR_QDA),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='qda_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_LSTM),\n",
    "#         oversample=None,\n",
    "#         lookback=lookback,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='lstm_os'\n",
    "#     )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress-fixed.pkl')\n",
    "par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', 'stress')\n",
    "\n",
    "if os.path.isdir(par_dir):\n",
    "    # Get a list of all the files in the folder\n",
    "    files = os.listdir(par_dir)\n",
    "\n",
    "    # Delete all the files in the folder\n",
    "    for file in files:\n",
    "        if file !='.ipynb_checkpoints':\n",
    "            os.remove(os.path.join(par_dir, file))\n",
    "os.makedirs(par_dir, exist_ok=True)\n",
    "\n",
    "#with on_ray(num_cpus=6):\n",
    "with on_ray():\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):       \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        ################################################\n",
    "        #Use mean threshold for all users (only training set,\\ \n",
    "        #we need to use raw value and binarize after data splitting)\n",
    "#         y =  LABELS_PROC['stress'].to_numpy()\n",
    "        #Use user speicifc mean threshold\n",
    "#         y =LABELS_PROC['stress_user_mean'].to_numpy()\n",
    "        #Use fixed threshold\n",
    "#         y =LABELS_PROC['stress_fixed'].to_numpy()\n",
    "        #Use three categories (fixed threshold) \n",
    "#        y =LABELS_PROC['stress_fixed_tri'].to_numpy()\n",
    "\n",
    "        \n",
    "        #The following code is designed for reordering for the sake of time series split\n",
    "        #################################################\n",
    "        # Create a DataFrame with user_id and datetime\n",
    "#         df = pd.DataFrame({'user_id': groups, 'datetime': pd.to_datetime(datetimes)})\n",
    "\n",
    "#         # Normalize the datetime for each user\n",
    "# #         df['datetime'] = df.groupby('user_id')['datetime'].transform(lambda x: x - x.min())\n",
    "#         df['datetime'] = df.groupby('user_id')['datetime'].transform(lambda x: x - x.min().normalize())\n",
    "\n",
    "#         # Sort the DataFrame by datetime\n",
    "# #         df = df.sort_values(by=['user_id', 'datetime'])\n",
    "#         df = df.sort_values(by=[ 'datetime'])\n",
    "#         # Shuffle the DataFrame\n",
    "# #         df = df.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "#         # Update groups and datetimes\n",
    "#         groups = df['user_id'].to_numpy()\n",
    "#         datetimes = df['datetime'].dt.total_seconds().to_numpy()  # convert to seconds\n",
    "\n",
    "#         # Use the new order to reorder X and y\n",
    "#         X = X.reindex(df.index)\n",
    "#         y = y[df.index]\n",
    "#         ###################################################\n",
    "        \n",
    "\n",
    "        #The following code is for only using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[~X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[~y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[~groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        #The following code is for excluding using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        \n",
    "        \n",
    "        ###########################################\n",
    "        #The following code is for similar-user model\n",
    "        ###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "        ###########################################\n",
    "        #Remove low frequency features\n",
    "#         mask = ['CAE#', 'MED#', 'ONF#', 'PWS#', 'RNG#','MSG#' ]\n",
    "#         X = X.loc[:, [all(m not in str(x) for m in mask) for x in X.columns]]\n",
    "\n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]\n",
    "\n",
    "\n",
    "\n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "        #The following code is for calculating aggregated features\n",
    "        ########################################################################\n",
    "        # Define a function to split the column name into sensor and attribute\n",
    "#         def split_column_name(col_name):\n",
    "#             parts = col_name.rsplit(\"#\", 1)  # Split on last occurrence of '#'\n",
    "#             return parts[0]  # This gives you 'Sensor#Attribute'\n",
    "\n",
    "#         # Get a list of unique sensor-attribute combinations\n",
    "#         df=feat_yesterday_sensor\n",
    "#         sensor_attributes = df.columns.map(split_column_name).unique()\n",
    "\n",
    "#         # Create a list to hold the aggregated results\n",
    "#         agg_results = []\n",
    "\n",
    "#         # Loop over each sensor-attribute, select the appropriate columns, compute the mean and std\n",
    "#         for sensor_attribute in sensor_attributes:\n",
    "#             # Select columns for this sensor-attribute\n",
    "#             cols_to_aggregate = [col for col in df.columns if col.startswith(sensor_attribute)]\n",
    "#             # Compute the mean and std and store in the new DataFrame\n",
    "#             agg_results.append(df[cols_to_aggregate].mean(axis=1).rename(sensor_attribute + '|'+ 'MEAN'))\n",
    "#             agg_results.append(df[cols_to_aggregate].std(axis=1).rename(sensor_attribute + '|'+'STD'))\n",
    "\n",
    "#         # Concatenate all the results into a single DataFrame\n",
    "#         agg_feature = pd.concat(agg_results, axis=1)\n",
    "        #########################################################################\n",
    "        feat_final = pd.concat([feat_baseline ],axis=1)\n",
    "        X = feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        \n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups, datetimes =datetimes,\n",
    "            path=par_dir,\n",
    "            normalize=True,\n",
    "            split='logo',\n",
    "            categories=cats,\n",
    "            split_params={'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, groups, t, datetimes = load(p)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "r = np.corrcoef(X['ESM#LastLabel'], y)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25318770878842567"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have two series: series1 and series2\n",
    "series1 = X['ESM#LastLabel']\n",
    "series2 = y\n",
    "# Create a scatter plot\n",
    "plt.scatter(series1, series2)\n",
    "plt.xlabel('Lagged Label Value (1st order)')\n",
    "plt.ylabel('Label Value')\n",
    "plt.title('1st Order Autocorrelation')\n",
    "\n",
    "# Add a trendline (optional)\n",
    "z = np.polyfit(series1, series2, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(series1, p(series1), color='r')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
