{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_low_variance(agg_feature, threshold=.0000001):\n",
    "    agg_feature_non_zero_var = agg_feature.loc[:,agg_feature.var()>threshold]\n",
    "    num_removed = agg_feature.shape[1]-agg_feature_non_zero_var.shape[1]\n",
    "    print(f'{num_removed}/{agg_feature.shape[1]} features with variance < {threshold} removed')\n",
    "    return agg_feature_non_zero_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def remove_pairwise_corr(agg_feature_percent_missing, PAIRWISE_CORR_THRESHOLD=0.8, outcome_variable=None):\n",
    "    if outcome_variable is not None:\n",
    "        outcome_variable = pd.Series(outcome_variable, index=agg_feature_percent_missing.index, name=\"outcome\")\n",
    "        corr_with_outcome = pd.merge(outcome_variable, agg_feature_percent_missing, left_index=True, right_index=True).corr()[outcome_variable.name].abs().sort_values(ascending=False)\n",
    "        importance_order = corr_with_outcome.index[1:].tolist()\n",
    "        agg_feature_percent_missing = agg_feature_percent_missing[importance_order]\n",
    "\n",
    "    Matrix = agg_feature_percent_missing.corr().abs()\n",
    "    \n",
    "    upper_triangle = Matrix.where(np.triu(np.ones(Matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    correlated_features = set()\n",
    "    for feature in upper_triangle.columns:\n",
    "        highly_correlated = upper_triangle[feature][upper_triangle[feature] > PAIRWISE_CORR_THRESHOLD].index\n",
    "        correlated_features.update(highly_correlated)\n",
    "\n",
    "    kept_features = list(set(agg_feature_percent_missing.columns) - correlated_features)\n",
    "    print(f\"Pairwise Corr: kept only {len(kept_features)}/{len(agg_feature_percent_missing.columns)} features\")\n",
    "    return agg_feature_percent_missing[kept_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LOGO + Timeseries k\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits):\n",
    "#         self.n_splits = n_splits\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         logo = LeaveOneGroupOut()\n",
    "\n",
    "#         for train_users, test_users in logo.split(X, y, groups):\n",
    "#             X_train_users, X_test_user = X.loc[train_users], X.loc[test_users]\n",
    "#             y_train_users, y_test_user = y[train_users], y[test_users]\n",
    "#             group_train_users, group_test_user = groups[train_users], groups[test_users]\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=self.n_splits) \n",
    "\n",
    "#             # only take the first split\n",
    "#             train_index, test_index = next(tscv.split(X_test_user))\n",
    "            \n",
    "#             X_train, X_test = pd.concat([X_train_users, X_test_user.iloc[train_index]]), X_test_user.iloc[test_index]\n",
    "#             y_train, y_test = np.concatenate([y_train_users, y_test_user[train_index]]), y_test_user[test_index]\n",
    "\n",
    "#             yield (X_train.index, X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logo + first 50% of data based on temporal order\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomCV:\n",
    "    def __init__(self, n_splits, test_ratio=0.9):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_ratio = test_ratio  # Include a test_ratio parameter\n",
    "\n",
    "    def split(self, X, y, groups):\n",
    "        logo = LeaveOneGroupOut()\n",
    "\n",
    "        for train_users, test_users in logo.split(X, y, groups):\n",
    "            # This now splits the test users' data based on the specified ratio.\n",
    "            X_train_users, X_test_user_full = X.loc[train_users], X.loc[test_users]\n",
    "            y_train_users, y_test_user_full = y[train_users], y[test_users]\n",
    "            groups_train_users, groups_test_user_full = groups[train_users], groups[test_users]\n",
    "            \n",
    "            # Determine the split index for the test user's data\n",
    "            split_index = int(len(X_test_user_full) * self.test_ratio)\n",
    "            \n",
    "            # Split the test user's data into the part used for training and the actual test set\n",
    "            X_test_user_train = X_test_user_full.iloc[:split_index]\n",
    "            y_test_user_train = y_test_user_full[:split_index]\n",
    "            X_test_user_test = X_test_user_full.iloc[split_index:]\n",
    "            y_test_user_test = y_test_user_full[split_index:]\n",
    "            \n",
    "            # Combine the other users' data with the part of the test user's data for training\n",
    "            X_train_combined = pd.concat([X_train_users, X_test_user_train])\n",
    "            y_train_combined = np.concatenate([y_train_users, y_test_user_train])\n",
    "\n",
    "            # Yield the combined training set and the test set for the current fold\n",
    "            yield (X_train_combined.index, X_test_user_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM\n",
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "    datetimes_train: np.ndarray = None\n",
    "    datetimes_test: np.ndarray = None\n",
    "\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    elif alg == 'TimeSeriesSplit':\n",
    "        splitter = TimeSeriesSplit(n_splits=n_splits)\n",
    "    elif alg == 'custom_cv':\n",
    "        splitter = CustomCV(n_splits=n_splits)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", \"TimeSeriesSplit\", \"custom_cv\" or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    datetimes_train: np.ndarray,  # Add datetimes_train parameter\n",
    "    datetimes_test: np.ndarray,  # Add datetimes_test parameter\n",
    "\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "\n",
    "\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "#         #Instead of using fixed threshold, we tried to use mean value as threshold for binarization\n",
    "# #     y_train_mean = np.mean(np.concatenate((y_train,y_test)))\n",
    "#     y_train_mean = np.mean(y_train)\n",
    "#     y_train = np.where(y_train > y_train_mean, 1, 0)\n",
    "#     y_test= np.where(y_test > y_train_mean, 1, 0)\n",
    "# #     X_train['ESM#LastLabel'] = np.where(X_train['ESM#LastLabel'] > y_train_mean, 1, 0)\n",
    "# #     X_test['ESM#LastLabel'] = np.where(X_test['ESM#LastLabel'] > y_train_mean, 1, 0)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "#         # Removing low variance features\n",
    "#         X_train = exclude_low_variance(X_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "#         #Removing highly correlated features\n",
    "#         X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "#         X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "#             # Create oversampled datetimes_train\n",
    "#             datetimes_train_oversampled = np.repeat(datetimes_train, sampler.sample_indices_.shape[0], axis=0)\n",
    "\n",
    "    # You can access the underlying model class like this:\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    datetimes:  np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "    \n",
    "    \n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "        datetimes_train, datetimes_test = datetimes[I_train], datetimes[I_test]  # Add datetimes_train and datetimes_test\n",
    "\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            \n",
    "            datetimes_train=datetimes_train,  # Pass datetimes_train\n",
    "            datetimes_test=datetimes_test,  # Pass datetimes_test\n",
    "\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        # After fitting, store the best iteration\n",
    "        self.best_iteration_ = self.model.get_booster().best_iteration\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "#         return self.model.predict(X)\n",
    "        return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "#         return self.model.predict_proba(X)\n",
    "        return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'proc', 'LABELS_PROC.csv'), index_col=['pcode','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 19:37:44,391\tINFO worker.py:1431 -- Connecting to existing Ray cluster at address: 192.168.1.28:6379...\n",
      "2023-11-07 19:37:44,400\tINFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] In progress: [xgb_os#1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] Success: [xgb_os#1] Normalizing numeric features (0.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] In progress: [xgb_os#1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] Success: [xgb_os#1] 1-th Feature selection (0.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 58; # Num. = 513)\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m - # Sel. Feat.: 184 (# Cat. = 6; # Num. = 178)\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] In progress: [xgb_os#1] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] Success: [xgb_os#1] Oversampling (0.42s).\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] In progress: [xgb_os#1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=28216)\u001b[0m [23-11-07 19:37:45] Success: [xgb_os#1] Training (0.22s).\n",
      "\u001b[2m\u001b[36m(_train pid=28426)\u001b[0m [23-11-07 19:37:45] In progress: [xgb_os#2] Normalizing numeric features.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28426)\u001b[0m [23-11-07 19:37:45] Success: [xgb_os#2] Normalizing numeric features (0.01s).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28426)\u001b[0m [23-11-07 19:37:45] In progress: [xgb_os#2] 1-th Feature selection.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28441)\u001b[0m [23-11-07 19:37:47] Success: [xgb_os#5] 1-th Feature selection (2.10s).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28441)\u001b[0m - # Orig. Feat.: 571 (# Cat. = 58; # Num. = 513)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28441)\u001b[0m - # Sel. Feat.: 370 (# Cat. = 28; # Num. = 342)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28441)\u001b[0m [23-11-07 19:37:47] In progress: [xgb_os#5] Oversampling.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28427)\u001b[0m [23-11-07 19:37:54] Success: [xgb_os#3] Oversampling (8.76s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28427)\u001b[0m [23-11-07 19:37:54] In progress: [xgb_os#3] Training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=28426)\u001b[0m [23-11-07 19:37:48] Success: [xgb_os#2] Training (0.63s).\n",
      "\u001b[2m\u001b[36m(_train pid=28427)\u001b[0m [23-11-07 19:37:55] Success: [xgb_os#3] Training (0.76s).\n",
      "\u001b[2m\u001b[36m(_train pid=28434)\u001b[0m [23-11-07 19:38:05] Success: [xgb_os#4] Oversampling (17.97s).\n",
      "\u001b[2m\u001b[36m(_train pid=28434)\u001b[0m [23-11-07 19:38:05] In progress: [xgb_os#4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=28434)\u001b[0m [23-11-07 19:38:06] Success: [xgb_os#4] Training (1.25s).\n",
      "\u001b[2m\u001b[36m(_train pid=28441)\u001b[0m [23-11-07 19:38:12] Success: [xgb_os#5] Oversampling (25.18s).\n",
      "\u001b[2m\u001b[36m(_train pid=28441)\u001b[0m [23-11-07 19:38:12] In progress: [xgb_os#5] Training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# ESTIMATOR_XGB = EvXGBClassifier(\n",
    "#     random_state=RANDOM_STATE, \n",
    "#     eval_metric='auc', \n",
    "#     eval_size=0.2, \n",
    "#     early_stopping_rounds=10, \n",
    "#     objective='binary:logistic', \n",
    "#     verbosity=0,\n",
    "#     learning_rate=0.01, \n",
    "#     colsample_bytree = 0.8,\n",
    "#     colsample_bylevel = 0.8,\n",
    "#     scale_pos_weight =2,\n",
    "#     min_child_weight =1,\n",
    "#     subsample = 0.8,\n",
    "#     max_depth =3,\n",
    "#     gamma =0.1,\n",
    "#     tree_method = 'hist',\n",
    "#     n_estimators =1000,\n",
    "#     reg_lambda = 1,\n",
    "#     reg_alpha= 1,\n",
    "#     num_parallel_tree =10\n",
    "\n",
    "# )\n",
    "\n",
    "ESTIMATOR_LR = LogisticRegression(random_state = RANDOM_STATE, max_iter=500 )\n",
    "ESTIMATOR_KNN = KNeighborsClassifier()\n",
    "ESTIMATOR_SVM = SVC(probability=True)\n",
    "ESTIMATOR_GP = GaussianProcessClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_DT = DecisionTreeClassifier(random_state = RANDOM_STATE)\n",
    "ESTIMATOR_MLP = MLPClassifier(random_state=RANDOM_STATE, max_iter=2000)\n",
    "ESTIMATOR_ADAB = AdaBoostClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_GNB = GaussianNB()\n",
    "ESTIMATOR_QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "  \n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "#     estimator=LinearSVC(\n",
    "#         penalty='l1',\n",
    "#         loss='squared_hinge',\n",
    "#         dual=False,\n",
    "#         tol=1e-3,\n",
    "#         C=1e-2,\n",
    "#         max_iter=5000,\n",
    "#         random_state=RANDOM_STATE\n",
    "#     ),\n",
    "#     threshold=1e-5\n",
    "    \n",
    "        estimator=LogisticRegression(\n",
    "        penalty='l1' \n",
    "        ,solver='liblinear'\n",
    "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "    ),\n",
    "    threshold = 0.005\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "\n",
    "SETTINGS = [\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_DUMMY),\n",
    "#         oversample=False,\n",
    "#         select=None,\n",
    "#         name='dummy'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_RF),\n",
    "#         oversample=False,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='rf_ns'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_RF),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='rf_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_XGB),\n",
    "#         oversample=False,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='xgb_ns'\n",
    "#     ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_LR),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='lr_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_KNN),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='knn_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_SVM),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='svm_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_DT),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='dt_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_MLP),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='mlp_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_ADAB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='adab_os'\n",
    "#     ),\n",
    "#         dict(\n",
    "#         estimator=clone(ESTIMATOR_GP),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gp_os'\n",
    "#     ),\n",
    "#             dict(\n",
    "#         estimator=clone(ESTIMATOR_GNB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gnb_os'\n",
    "#     ),\n",
    "#             dict(\n",
    "#         estimator=clone(ESTIMATOR_QDA),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='qda_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_LSTM),\n",
    "#         oversample=None,\n",
    "#         lookback=lookback,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='lstm_os'\n",
    "#     )\n",
    "]\n",
    "\n",
    "\n",
    "#The following is dataset for normalization for each user\n",
    "# p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress-fixed-normalized.pkl')\n",
    "#The following is dataset without normalization for each user\n",
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress-fixed.pkl')\n",
    "\n",
    "par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', 'stress')\n",
    "\n",
    "if os.path.isdir(par_dir):\n",
    "    # Get a list of all the files in the folder\n",
    "    files = os.listdir(par_dir)\n",
    "\n",
    "    # Delete all the files in the folder\n",
    "    for file in files:\n",
    "        if file !='.ipynb_checkpoints':\n",
    "            os.remove(os.path.join(par_dir, file))\n",
    "os.makedirs(par_dir, exist_ok=True)\n",
    "\n",
    "#with on_ray(num_cpus=6):\n",
    "with on_ray():\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):       \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        ################################################\n",
    "        #Use mean threshold for all users (only training set,\\ \n",
    "        #we need to use raw value and binarize after data splitting)\n",
    "#         y =  LABELS_PROC['stress'].to_numpy()\n",
    "        #Use user speicifc mean threshold\n",
    "#         y =LABELS_PROC['stress_user_mean'].to_numpy()\n",
    "        #Use fixed threshold\n",
    "#         y =LABELS_PROC['stress_fixed'].to_numpy()\n",
    "        #Use three categories (fixed threshold) \n",
    "#        y =LABELS_PROC['stress_fixed_tri'].to_numpy()\n",
    "\n",
    "        \n",
    "        #The following code is designed for reordering for the sake of time series split\n",
    "        #################################################\n",
    "#         # Create a DataFrame with user_id and datetime\n",
    "#         df = pd.DataFrame({'user_id': groups, 'datetime': pd.to_datetime(datetimes)})\n",
    "\n",
    "#         # Normalize the datetime for each user\n",
    "# #         df['datetime'] = df.groupby('user_id')['datetime'].transform(lambda x: x - x.min())\n",
    "#         df['datetime'] = df.groupby('user_id')['datetime'].transform(lambda x: x - x.min().normalize())\n",
    "\n",
    "#         # Sort the DataFrame by datetime\n",
    "# #         df = df.sort_values(by=['user_id', 'datetime'])\n",
    "#         df = df.sort_values(by=[ 'datetime'])\n",
    "#         # Shuffle the DataFrame\n",
    "# #         df = df.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "#         # Update groups and datetimes\n",
    "#         groups = df['user_id'].to_numpy()\n",
    "#         datetimes = df['datetime'].dt.total_seconds().to_numpy()  # convert to seconds\n",
    "\n",
    "#         # Use the new order to reorder X and y\n",
    "#         X = X.reindex(df.index)\n",
    "#         y = y[df.index]\n",
    "#         ###################################################\n",
    "#         #The following is for ordering the temporal order for each user without normalization of timestamp\n",
    "\n",
    "#         # Assuming 'groups' and 'datetimes' are defined and 'X' is a Pandas DataFrame\n",
    "#         # Also assuming 'y' is a numpy array that corresponds to 'X'\n",
    "\n",
    "#         # Create a DataFrame with user_id and datetime\n",
    "#         df = pd.DataFrame({'user_id': groups, 'datetime': pd.to_datetime(datetimes)})\n",
    "\n",
    "#         # Sort the DataFrame by user_id and datetime\n",
    "#         df = df.sort_values(by=['user_id', 'datetime'])\n",
    "\n",
    "#         # Convert y to a pandas Series with the same index as df to allow for reindexing\n",
    "#         y_series = pd.Series(y, index=df.index)\n",
    "\n",
    "#         # Reorder X to align with the sorted DataFrame's index\n",
    "#         X_sorted = X.reindex(df.index)\n",
    "\n",
    "#         # Now, reorder y to align with the sorted DataFrame's index using the Series created\n",
    "#         y_sorted = y_series.reindex(df.index).values  # Convert back to numpy array if needed\n",
    "\n",
    "#         # Update the groups and datetimes using the sorted DataFrame\n",
    "#         groups_sorted = df['user_id'].values\n",
    "#         datetimes_sorted = df['datetime'].values  # Assuming you want to keep the datetime objects\n",
    "\n",
    "#         # If you require the datetime in numeric format without normalization, convert as follows:\n",
    "#         df['timestamp'] = df['datetime'].astype('int64') // 10**9  # Unix timestamp in seconds\n",
    "#         datetimes_sorted_as_timestamp = df['timestamp'].values\n",
    "\n",
    "#         # Now, X_sorted, y_sorted, groups_sorted, and either datetimes_sorted or datetimes_sorted_as_timestamp\n",
    "#         # are all aligned and sorted by 'user_id' and 'datetime'.\n",
    "\n",
    "        \n",
    "\n",
    "        #The following code is for only using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[~X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[~y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[~groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        #The following code is for excluding using 1st day\n",
    "        ###########################################\n",
    "#         filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "#         X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "#         y_series = pd.Series(y, index=X.index)\n",
    "#         y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "#         y_filtered = y_filtered.values\n",
    "#         groups_series = pd.Series(groups, index=X.index)\n",
    "#         groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "#         groups_filtered = groups_filtered.values\n",
    "#         X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        \n",
    "        \n",
    "        ###########################################\n",
    "        #The following code is for similar-user model\n",
    "        ###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "        ###########################################\n",
    "        #Remove low frequency features\n",
    "#         mask = ['CAE#', 'MED#', 'ONF#', 'PWS#', 'RNG#','MSG#' ]\n",
    "#         X = X.loc[:, [all(m not in str(x) for m in mask) for x in X.columns]]\n",
    "\n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]\n",
    "\n",
    "\n",
    "\n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "        #The following code is for calculating aggregated features\n",
    "        ########################################################################\n",
    "#         # Define a function to split the column name into sensor and attribute\n",
    "#         def split_column_name(col_name):\n",
    "#             parts = col_name.rsplit(\"#\", 1)  # Split on last occurrence of '#'\n",
    "#             return parts[0]  # This gives you 'Sensor#Attribute'\n",
    "\n",
    "#         # Get a list of unique sensor-attribute combinations\n",
    "#         df=feat_yesterday_sensor\n",
    "#         sensor_attributes = df.columns.map(split_column_name).unique()\n",
    "\n",
    "#         # Create a list to hold the aggregated results\n",
    "#         agg_results = []\n",
    "\n",
    "#         # Loop over each sensor-attribute, select the appropriate columns, compute the mean and std\n",
    "#         for sensor_attribute in sensor_attributes:\n",
    "#             # Select columns for this sensor-attribute\n",
    "#             cols_to_aggregate = [col for col in df.columns if col.startswith(sensor_attribute)]\n",
    "#             # Compute the mean and std and store in the new DataFrame\n",
    "#             agg_results.append(df[cols_to_aggregate].mean(axis=1).rename(sensor_attribute + '|'+ 'MEAN'))\n",
    "#             agg_results.append(df[cols_to_aggregate].std(axis=1).rename(sensor_attribute + '|'+'STD'))\n",
    "\n",
    "#         # Concatenate all the results into a single DataFrame\n",
    "#         agg_feature = pd.concat(agg_results, axis=1)\n",
    "\n",
    "        ######################################################################\n",
    "        feat_final = pd.concat([ feat_baseline ],axis=1)\n",
    "        \n",
    "#         # Fill NaN values with zeros\n",
    "#         feat_final = feat_final.fillna(0)\n",
    "\n",
    "#         # Find the maximum non-infinity value and minimum non-negative infinity value across the entire dataframe\n",
    "#         max_val = feat_final[feat_final != np.inf].max().max()\n",
    "#         min_val = feat_final[feat_final != -np.inf].min().min()\n",
    "\n",
    "#         # Replace positive and negative infinity values\n",
    "#         feat_final.replace(np.inf, max_val, inplace=True)\n",
    "#         feat_final.replace(-np.inf, min_val, inplace=True)\n",
    "        \n",
    "        X = feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        \n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups, datetimes =datetimes,\n",
    "            path=par_dir,\n",
    "            normalize=True,\n",
    "            split='TimeSeriesSplit',\n",
    "            categories=cats,\n",
    "            split_params={'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, groups, t, datetimes = load(p)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "r = np.corrcoef(X['ESM#LastLabel'], y)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have two series: series1 and series2\n",
    "series1 = X['ESM#LastLabel']\n",
    "series2 = y\n",
    "# Create a scatter plot\n",
    "plt.scatter(series1, series2)\n",
    "plt.xlabel('Lagged Label Value (1st order)')\n",
    "plt.ylabel('Label Value')\n",
    "plt.title('1st Order Autocorrelation')\n",
    "\n",
    "# Add a trendline (optional)\n",
    "z = np.polyfit(series1, series2, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(series1, p(series1), color='r')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
